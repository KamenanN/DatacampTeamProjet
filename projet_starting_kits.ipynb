{"cells":[{"cell_type":"code","execution_count":44,"metadata":{},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import optuna\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import OneHotEncoder\n","import pickle\n","\n","#Test de plusieurs modèles\n","from sklearn.linear_model import LinearRegression\n","from sklearn.linear_model import Ridge\n","from sklearn.linear_model import Lasso\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.ensemble import GradientBoostingRegressor\n","from sklearn.linear_model import ElasticNet\n","from sklearn.linear_model import LinearRegression\n","from sklearn.metrics import mean_squared_error, r2_score"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","ByCity= pd.read_csv('./Data/GlobalLandTemperaturesByCity.csv')\n","ByState= pd.read_csv('./Data/GlobalLandTemperaturesByCity.csv')\n","ByCountry= pd.read_csv('./Data/GlobalLandTemperaturesByCountry.csv')\n","ByMajorCity= pd.read_csv('./Data/GlobalLandTemperaturesByMajorCity.csv')\n","ByAll= pd.read_csv('./Data/GlobalTemperatures.csv')"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["def count_nan(dataframe):\n","    # Calculer le nombre total de NaN dans le DataFrame\n","    total_nan = dataframe.isna().sum().sum()\n","\n","    # Calculer le nombre de NaN par colonne\n","    nan_per_column = dataframe.isna().sum()\n","\n","    return total_nan, nan_per_column"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Shape de las matrice:  (239177, 7)\n","Nombre total de NaN dans le DataFrame : 22004\n","Nombre de NaN par colonne :\n","dt                                   0\n","AverageTemperature               11002\n","AverageTemperatureUncertainty    11002\n","City                                 0\n","Country                              0\n","Latitude                             0\n","Longitude                            0\n","dtype: int64\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>dt</th>\n","      <th>AverageTemperature</th>\n","      <th>AverageTemperatureUncertainty</th>\n","      <th>City</th>\n","      <th>Country</th>\n","      <th>Latitude</th>\n","      <th>Longitude</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1849-01-01</td>\n","      <td>26.704</td>\n","      <td>1.435</td>\n","      <td>Abidjan</td>\n","      <td>Côte D'Ivoire</td>\n","      <td>5.63N</td>\n","      <td>3.23W</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1849-02-01</td>\n","      <td>27.434</td>\n","      <td>1.362</td>\n","      <td>Abidjan</td>\n","      <td>Côte D'Ivoire</td>\n","      <td>5.63N</td>\n","      <td>3.23W</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1849-03-01</td>\n","      <td>28.101</td>\n","      <td>1.612</td>\n","      <td>Abidjan</td>\n","      <td>Côte D'Ivoire</td>\n","      <td>5.63N</td>\n","      <td>3.23W</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1849-04-01</td>\n","      <td>26.140</td>\n","      <td>1.387</td>\n","      <td>Abidjan</td>\n","      <td>Côte D'Ivoire</td>\n","      <td>5.63N</td>\n","      <td>3.23W</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1849-05-01</td>\n","      <td>25.427</td>\n","      <td>1.200</td>\n","      <td>Abidjan</td>\n","      <td>Côte D'Ivoire</td>\n","      <td>5.63N</td>\n","      <td>3.23W</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>239172</th>\n","      <td>2013-05-01</td>\n","      <td>18.979</td>\n","      <td>0.807</td>\n","      <td>Xian</td>\n","      <td>China</td>\n","      <td>34.56N</td>\n","      <td>108.97E</td>\n","    </tr>\n","    <tr>\n","      <th>239173</th>\n","      <td>2013-06-01</td>\n","      <td>23.522</td>\n","      <td>0.647</td>\n","      <td>Xian</td>\n","      <td>China</td>\n","      <td>34.56N</td>\n","      <td>108.97E</td>\n","    </tr>\n","    <tr>\n","      <th>239174</th>\n","      <td>2013-07-01</td>\n","      <td>25.251</td>\n","      <td>1.042</td>\n","      <td>Xian</td>\n","      <td>China</td>\n","      <td>34.56N</td>\n","      <td>108.97E</td>\n","    </tr>\n","    <tr>\n","      <th>239175</th>\n","      <td>2013-08-01</td>\n","      <td>24.528</td>\n","      <td>0.840</td>\n","      <td>Xian</td>\n","      <td>China</td>\n","      <td>34.56N</td>\n","      <td>108.97E</td>\n","    </tr>\n","    <tr>\n","      <th>239176</th>\n","      <td>2013-09-01</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Xian</td>\n","      <td>China</td>\n","      <td>34.56N</td>\n","      <td>108.97E</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>239177 rows × 7 columns</p>\n","</div>"],"text/plain":["                dt  AverageTemperature  AverageTemperatureUncertainty  \\\n","0       1849-01-01              26.704                          1.435   \n","1       1849-02-01              27.434                          1.362   \n","2       1849-03-01              28.101                          1.612   \n","3       1849-04-01              26.140                          1.387   \n","4       1849-05-01              25.427                          1.200   \n","...            ...                 ...                            ...   \n","239172  2013-05-01              18.979                          0.807   \n","239173  2013-06-01              23.522                          0.647   \n","239174  2013-07-01              25.251                          1.042   \n","239175  2013-08-01              24.528                          0.840   \n","239176  2013-09-01                 NaN                            NaN   \n","\n","           City        Country Latitude Longitude  \n","0       Abidjan  Côte D'Ivoire    5.63N     3.23W  \n","1       Abidjan  Côte D'Ivoire    5.63N     3.23W  \n","2       Abidjan  Côte D'Ivoire    5.63N     3.23W  \n","3       Abidjan  Côte D'Ivoire    5.63N     3.23W  \n","4       Abidjan  Côte D'Ivoire    5.63N     3.23W  \n","...         ...            ...      ...       ...  \n","239172     Xian          China   34.56N   108.97E  \n","239173     Xian          China   34.56N   108.97E  \n","239174     Xian          China   34.56N   108.97E  \n","239175     Xian          China   34.56N   108.97E  \n","239176     Xian          China   34.56N   108.97E  \n","\n","[239177 rows x 7 columns]"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["total_nan, nan_per_column = count_nan(ByMajorCity)\n","\n","print(\"Shape de las matrice: \", ByMajorCity.shape)\n","print(f\"Nombre total de NaN dans le DataFrame : {total_nan}\")\n","print(\"Nombre de NaN par colonne :\")\n","print(nan_per_column)\n","\n","ByMajorCity"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["ByMajorCity_sans_NA=ByMajorCity.dropna()"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\valentin\\AppData\\Local\\Temp\\ipykernel_1688\\1975194507.py:1: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  ByMajorCity_sans_NA['dt'] = pd.to_datetime(ByMajorCity_sans_NA['dt'])\n"]}],"source":["ByMajorCity_sans_NA['dt'] = pd.to_datetime(ByMajorCity_sans_NA['dt'])"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["#Here we drop the City column because we consider that the coordinates already provide a similar information and the City feature contains also a lot of values\n","#We also drop the 'AverageTemperatureUncertainty' feature because of its close relation to the target variable\n","ByMajorCity_true=ByMajorCity_sans_NA.drop(columns=['City','AverageTemperatureUncertainty'])"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["# Initialize the LabelEncoder\n","from sklearn.preprocessing import LabelEncoder\n","label_encoder = LabelEncoder()\n","\n","# Fit and transform the 'Country' column\n","ByMajorCity_true['Country'] = label_encoder.fit_transform(ByMajorCity_true['Country'])"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>dt</th>\n","      <th>AverageTemperature</th>\n","      <th>Country</th>\n","      <th>Latitude</th>\n","      <th>Longitude</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1849-01-01</td>\n","      <td>26.704</td>\n","      <td>11</td>\n","      <td>5.63N</td>\n","      <td>3.23W</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1849-02-01</td>\n","      <td>27.434</td>\n","      <td>11</td>\n","      <td>5.63N</td>\n","      <td>3.23W</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1849-03-01</td>\n","      <td>28.101</td>\n","      <td>11</td>\n","      <td>5.63N</td>\n","      <td>3.23W</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1849-04-01</td>\n","      <td>26.140</td>\n","      <td>11</td>\n","      <td>5.63N</td>\n","      <td>3.23W</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1849-05-01</td>\n","      <td>25.427</td>\n","      <td>11</td>\n","      <td>5.63N</td>\n","      <td>3.23W</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>239171</th>\n","      <td>2013-04-01</td>\n","      <td>12.563</td>\n","      <td>8</td>\n","      <td>34.56N</td>\n","      <td>108.97E</td>\n","    </tr>\n","    <tr>\n","      <th>239172</th>\n","      <td>2013-05-01</td>\n","      <td>18.979</td>\n","      <td>8</td>\n","      <td>34.56N</td>\n","      <td>108.97E</td>\n","    </tr>\n","    <tr>\n","      <th>239173</th>\n","      <td>2013-06-01</td>\n","      <td>23.522</td>\n","      <td>8</td>\n","      <td>34.56N</td>\n","      <td>108.97E</td>\n","    </tr>\n","    <tr>\n","      <th>239174</th>\n","      <td>2013-07-01</td>\n","      <td>25.251</td>\n","      <td>8</td>\n","      <td>34.56N</td>\n","      <td>108.97E</td>\n","    </tr>\n","    <tr>\n","      <th>239175</th>\n","      <td>2013-08-01</td>\n","      <td>24.528</td>\n","      <td>8</td>\n","      <td>34.56N</td>\n","      <td>108.97E</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>228175 rows × 5 columns</p>\n","</div>"],"text/plain":["               dt  AverageTemperature  Country Latitude Longitude\n","0      1849-01-01              26.704       11    5.63N     3.23W\n","1      1849-02-01              27.434       11    5.63N     3.23W\n","2      1849-03-01              28.101       11    5.63N     3.23W\n","3      1849-04-01              26.140       11    5.63N     3.23W\n","4      1849-05-01              25.427       11    5.63N     3.23W\n","...           ...                 ...      ...      ...       ...\n","239171 2013-04-01              12.563        8   34.56N   108.97E\n","239172 2013-05-01              18.979        8   34.56N   108.97E\n","239173 2013-06-01              23.522        8   34.56N   108.97E\n","239174 2013-07-01              25.251        8   34.56N   108.97E\n","239175 2013-08-01              24.528        8   34.56N   108.97E\n","\n","[228175 rows x 5 columns]"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["# ByMajorCity_true = ByMajorCity_true.drop(ByMajorCity_true.columns[-1], axis=1)\n","ByMajorCity_true"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[],"source":["By_Major_City_numpy=np.array(ByMajorCity_true)\n","Years=[By_Major_City_numpy[k,0].year for k in range(len(By_Major_City_numpy))]\n","Month=[By_Major_City_numpy[k,0].month for k in range(len(By_Major_City_numpy))]"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>dt</th>\n","      <th>AverageTemperature</th>\n","      <th>Country</th>\n","      <th>Latitude</th>\n","      <th>Longitude</th>\n","      <th>Year</th>\n","      <th>Month</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1849-01-01</td>\n","      <td>26.704</td>\n","      <td>11</td>\n","      <td>5.63</td>\n","      <td>3.23</td>\n","      <td>1849</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1849-02-01</td>\n","      <td>27.434</td>\n","      <td>11</td>\n","      <td>5.63</td>\n","      <td>3.23</td>\n","      <td>1849</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1849-03-01</td>\n","      <td>28.101</td>\n","      <td>11</td>\n","      <td>5.63</td>\n","      <td>3.23</td>\n","      <td>1849</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1849-04-01</td>\n","      <td>26.140</td>\n","      <td>11</td>\n","      <td>5.63</td>\n","      <td>3.23</td>\n","      <td>1849</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1849-05-01</td>\n","      <td>25.427</td>\n","      <td>11</td>\n","      <td>5.63</td>\n","      <td>3.23</td>\n","      <td>1849</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>239171</th>\n","      <td>2013-04-01</td>\n","      <td>12.563</td>\n","      <td>8</td>\n","      <td>34.56</td>\n","      <td>108.97</td>\n","      <td>2013</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>239172</th>\n","      <td>2013-05-01</td>\n","      <td>18.979</td>\n","      <td>8</td>\n","      <td>34.56</td>\n","      <td>108.97</td>\n","      <td>2013</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>239173</th>\n","      <td>2013-06-01</td>\n","      <td>23.522</td>\n","      <td>8</td>\n","      <td>34.56</td>\n","      <td>108.97</td>\n","      <td>2013</td>\n","      <td>6</td>\n","    </tr>\n","    <tr>\n","      <th>239174</th>\n","      <td>2013-07-01</td>\n","      <td>25.251</td>\n","      <td>8</td>\n","      <td>34.56</td>\n","      <td>108.97</td>\n","      <td>2013</td>\n","      <td>7</td>\n","    </tr>\n","    <tr>\n","      <th>239175</th>\n","      <td>2013-08-01</td>\n","      <td>24.528</td>\n","      <td>8</td>\n","      <td>34.56</td>\n","      <td>108.97</td>\n","      <td>2013</td>\n","      <td>8</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>228175 rows × 7 columns</p>\n","</div>"],"text/plain":["               dt  AverageTemperature  Country  Latitude  Longitude  Year  \\\n","0      1849-01-01              26.704       11      5.63       3.23  1849   \n","1      1849-02-01              27.434       11      5.63       3.23  1849   \n","2      1849-03-01              28.101       11      5.63       3.23  1849   \n","3      1849-04-01              26.140       11      5.63       3.23  1849   \n","4      1849-05-01              25.427       11      5.63       3.23  1849   \n","...           ...                 ...      ...       ...        ...   ...   \n","239171 2013-04-01              12.563        8     34.56     108.97  2013   \n","239172 2013-05-01              18.979        8     34.56     108.97  2013   \n","239173 2013-06-01              23.522        8     34.56     108.97  2013   \n","239174 2013-07-01              25.251        8     34.56     108.97  2013   \n","239175 2013-08-01              24.528        8     34.56     108.97  2013   \n","\n","        Month  \n","0           1  \n","1           2  \n","2           3  \n","3           4  \n","4           5  \n","...       ...  \n","239171      4  \n","239172      5  \n","239173      6  \n","239174      7  \n","239175      8  \n","\n","[228175 rows x 7 columns]"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["ByMajorCity_true['Year']=Years\n","ByMajorCity_true['Month']=Month\n","ByMajorCity_true['Latitude'] = ByMajorCity_true['Latitude'].str[:-1]\n","ByMajorCity_true['Latitude'] = ByMajorCity_true['Latitude'].astype(float)\n","\n","ByMajorCity_true['Longitude'] = ByMajorCity_true['Longitude'].str[:-1]\n","ByMajorCity_true['Longitude'] = ByMajorCity_true['Longitude'].astype(float)\n","ByMajorCity_true"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[],"source":["#Maintenant on lance le train_test_split\n","from sklearn.model_selection import train_test_split\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.metrics import mean_squared_error\n","\n","# Diviser les données en features (X) et target (y)\n","X = ByMajorCity_true.drop(columns=['AverageTemperature','dt'])\n","y = ByMajorCity_true['AverageTemperature']\n","\n","# Diviser les données en ensemble d'entraînement et ensemble de test\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"]},{"cell_type":"markdown","metadata":{},"source":["# Test de modèles"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[],"source":["#Fonction pour afficher les poids\n","def affichage_poids(model):\n","    # Obtention des poids\n","    feature_weights = model.coef_\n","\n","    # Create a dictionary to pair feature names with their weights\n","    feature_weights_dict = dict(zip(X.columns.to_list(), feature_weights))\n","    # Assuming you've already created the feature_weights_dict\n","    feature_weights_df = pd.DataFrame(feature_weights_dict.items(), columns=['Variable', 'Poids'])\n","\n","    transposed_df = feature_weights_df.transpose()\n","\n","    # Display the DataFrame\n","    print('\\n', transposed_df.to_string())"]},{"cell_type":"markdown","metadata":{},"source":["# Logistic Regression"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["[I 2024-02-12 18:55:19,772] A new study created in memory with name: no-name-a0d99d14-09dd-4e90-9ff2-093856724908\n"]},{"name":"stderr","output_type":"stream","text":["[I 2024-02-12 18:55:19,931] Trial 0 finished with value: 61.102298786944246 and parameters: {'fit_intercept': False, 'positive': False}. Best is trial 0 with value: 61.102298786944246.\n","[I 2024-02-12 18:55:20,053] Trial 1 finished with value: 97.54090497323931 and parameters: {'fit_intercept': True, 'positive': True}. Best is trial 0 with value: 61.102298786944246.\n","[I 2024-02-12 18:55:20,180] Trial 2 finished with value: 98.45722051683858 and parameters: {'fit_intercept': False, 'positive': True}. Best is trial 0 with value: 61.102298786944246.\n","[I 2024-02-12 18:55:20,350] Trial 3 finished with value: 60.6011097307731 and parameters: {'fit_intercept': True, 'positive': False}. Best is trial 3 with value: 60.6011097307731.\n","[I 2024-02-12 18:55:20,494] Trial 4 finished with value: 98.45722051683858 and parameters: {'fit_intercept': False, 'positive': True}. Best is trial 3 with value: 60.6011097307731.\n","[I 2024-02-12 18:55:20,636] Trial 5 finished with value: 98.45722051683858 and parameters: {'fit_intercept': False, 'positive': True}. Best is trial 3 with value: 60.6011097307731.\n","[I 2024-02-12 18:55:20,774] Trial 6 finished with value: 98.45722051683858 and parameters: {'fit_intercept': False, 'positive': True}. Best is trial 3 with value: 60.6011097307731.\n","[I 2024-02-12 18:55:20,964] Trial 7 finished with value: 61.102298786944246 and parameters: {'fit_intercept': False, 'positive': False}. Best is trial 3 with value: 60.6011097307731.\n","[I 2024-02-12 18:55:21,105] Trial 8 finished with value: 97.54090497323931 and parameters: {'fit_intercept': True, 'positive': True}. Best is trial 3 with value: 60.6011097307731.\n","[I 2024-02-12 18:55:21,307] Trial 9 finished with value: 60.6011097307731 and parameters: {'fit_intercept': True, 'positive': False}. Best is trial 3 with value: 60.6011097307731.\n","[I 2024-02-12 18:55:21,492] Trial 10 finished with value: 60.6011097307731 and parameters: {'fit_intercept': True, 'positive': False}. Best is trial 3 with value: 60.6011097307731.\n","[I 2024-02-12 18:55:21,699] Trial 11 finished with value: 60.6011097307731 and parameters: {'fit_intercept': True, 'positive': False}. Best is trial 3 with value: 60.6011097307731.\n","[I 2024-02-12 18:55:21,868] Trial 12 finished with value: 60.6011097307731 and parameters: {'fit_intercept': True, 'positive': False}. Best is trial 3 with value: 60.6011097307731.\n","[I 2024-02-12 18:55:22,074] Trial 13 finished with value: 60.6011097307731 and parameters: {'fit_intercept': True, 'positive': False}. Best is trial 3 with value: 60.6011097307731.\n","[I 2024-02-12 18:55:22,290] Trial 14 finished with value: 60.6011097307731 and parameters: {'fit_intercept': True, 'positive': False}. Best is trial 3 with value: 60.6011097307731.\n","[I 2024-02-12 18:55:22,476] Trial 15 finished with value: 60.6011097307731 and parameters: {'fit_intercept': True, 'positive': False}. Best is trial 3 with value: 60.6011097307731.\n","[I 2024-02-12 18:55:22,666] Trial 16 finished with value: 60.6011097307731 and parameters: {'fit_intercept': True, 'positive': False}. Best is trial 3 with value: 60.6011097307731.\n","[I 2024-02-12 18:55:22,835] Trial 17 finished with value: 60.6011097307731 and parameters: {'fit_intercept': True, 'positive': False}. Best is trial 3 with value: 60.6011097307731.\n","[I 2024-02-12 18:55:23,037] Trial 18 finished with value: 60.6011097307731 and parameters: {'fit_intercept': True, 'positive': False}. Best is trial 3 with value: 60.6011097307731.\n","[I 2024-02-12 18:55:23,293] Trial 19 finished with value: 60.6011097307731 and parameters: {'fit_intercept': True, 'positive': False}. Best is trial 3 with value: 60.6011097307731.\n","[I 2024-02-12 18:55:23,488] Trial 20 finished with value: 60.6011097307731 and parameters: {'fit_intercept': True, 'positive': False}. Best is trial 3 with value: 60.6011097307731.\n","[I 2024-02-12 18:55:23,827] Trial 21 finished with value: 60.6011097307731 and parameters: {'fit_intercept': True, 'positive': False}. Best is trial 3 with value: 60.6011097307731.\n","[I 2024-02-12 18:55:24,018] Trial 22 finished with value: 60.6011097307731 and parameters: {'fit_intercept': True, 'positive': False}. Best is trial 3 with value: 60.6011097307731.\n","[I 2024-02-12 18:55:24,203] Trial 23 finished with value: 60.6011097307731 and parameters: {'fit_intercept': True, 'positive': False}. Best is trial 3 with value: 60.6011097307731.\n","[I 2024-02-12 18:55:24,387] Trial 24 finished with value: 60.6011097307731 and parameters: {'fit_intercept': True, 'positive': False}. Best is trial 3 with value: 60.6011097307731.\n","[I 2024-02-12 18:55:24,574] Trial 25 finished with value: 60.6011097307731 and parameters: {'fit_intercept': True, 'positive': False}. Best is trial 3 with value: 60.6011097307731.\n","[I 2024-02-12 18:55:24,776] Trial 26 finished with value: 60.6011097307731 and parameters: {'fit_intercept': True, 'positive': False}. Best is trial 3 with value: 60.6011097307731.\n","[I 2024-02-12 18:55:24,977] Trial 27 finished with value: 60.6011097307731 and parameters: {'fit_intercept': True, 'positive': False}. Best is trial 3 with value: 60.6011097307731.\n","[I 2024-02-12 18:55:25,165] Trial 28 finished with value: 60.6011097307731 and parameters: {'fit_intercept': True, 'positive': False}. Best is trial 3 with value: 60.6011097307731.\n","[I 2024-02-12 18:55:25,335] Trial 29 finished with value: 61.102298786944246 and parameters: {'fit_intercept': False, 'positive': False}. Best is trial 3 with value: 60.6011097307731.\n","[I 2024-02-12 18:55:25,521] Trial 30 finished with value: 60.6011097307731 and parameters: {'fit_intercept': True, 'positive': False}. Best is trial 3 with value: 60.6011097307731.\n","[I 2024-02-12 18:55:25,723] Trial 31 finished with value: 60.6011097307731 and parameters: {'fit_intercept': True, 'positive': False}. Best is trial 3 with value: 60.6011097307731.\n","[I 2024-02-12 18:55:25,914] Trial 32 finished with value: 60.6011097307731 and parameters: {'fit_intercept': True, 'positive': False}. Best is trial 3 with value: 60.6011097307731.\n","[I 2024-02-12 18:55:26,085] Trial 33 finished with value: 60.6011097307731 and parameters: {'fit_intercept': True, 'positive': False}. Best is trial 3 with value: 60.6011097307731.\n","[I 2024-02-12 18:55:26,223] Trial 34 finished with value: 97.54090497323931 and parameters: {'fit_intercept': True, 'positive': True}. Best is trial 3 with value: 60.6011097307731.\n","[I 2024-02-12 18:55:26,381] Trial 35 finished with value: 98.45722051683858 and parameters: {'fit_intercept': False, 'positive': True}. Best is trial 3 with value: 60.6011097307731.\n","[I 2024-02-12 18:55:26,552] Trial 36 finished with value: 60.6011097307731 and parameters: {'fit_intercept': True, 'positive': False}. Best is trial 3 with value: 60.6011097307731.\n","[I 2024-02-12 18:55:26,690] Trial 37 finished with value: 98.45722051683858 and parameters: {'fit_intercept': False, 'positive': True}. Best is trial 3 with value: 60.6011097307731.\n","[I 2024-02-12 18:55:26,878] Trial 38 finished with value: 60.6011097307731 and parameters: {'fit_intercept': True, 'positive': False}. Best is trial 3 with value: 60.6011097307731.\n","[I 2024-02-12 18:55:27,003] Trial 39 finished with value: 98.45722051683858 and parameters: {'fit_intercept': False, 'positive': True}. Best is trial 3 with value: 60.6011097307731.\n","[I 2024-02-12 18:55:27,190] Trial 40 finished with value: 60.6011097307731 and parameters: {'fit_intercept': True, 'positive': False}. Best is trial 3 with value: 60.6011097307731.\n","[I 2024-02-12 18:55:27,381] Trial 41 finished with value: 60.6011097307731 and parameters: {'fit_intercept': True, 'positive': False}. Best is trial 3 with value: 60.6011097307731.\n","[I 2024-02-12 18:55:27,551] Trial 42 finished with value: 60.6011097307731 and parameters: {'fit_intercept': True, 'positive': False}. Best is trial 3 with value: 60.6011097307731.\n","[I 2024-02-12 18:55:27,756] Trial 43 finished with value: 60.6011097307731 and parameters: {'fit_intercept': True, 'positive': False}. Best is trial 3 with value: 60.6011097307731.\n","[I 2024-02-12 18:55:27,971] Trial 44 finished with value: 60.6011097307731 and parameters: {'fit_intercept': True, 'positive': False}. Best is trial 3 with value: 60.6011097307731.\n","[I 2024-02-12 18:55:28,159] Trial 45 finished with value: 60.6011097307731 and parameters: {'fit_intercept': True, 'positive': False}. Best is trial 3 with value: 60.6011097307731.\n","[I 2024-02-12 18:55:28,351] Trial 46 finished with value: 60.6011097307731 and parameters: {'fit_intercept': True, 'positive': False}. Best is trial 3 with value: 60.6011097307731.\n","[I 2024-02-12 18:55:28,485] Trial 47 finished with value: 98.45722051683858 and parameters: {'fit_intercept': False, 'positive': True}. Best is trial 3 with value: 60.6011097307731.\n","[I 2024-02-12 18:55:28,670] Trial 48 finished with value: 60.6011097307731 and parameters: {'fit_intercept': True, 'positive': False}. Best is trial 3 with value: 60.6011097307731.\n","[I 2024-02-12 18:55:28,842] Trial 49 finished with value: 60.6011097307731 and parameters: {'fit_intercept': True, 'positive': False}. Best is trial 3 with value: 60.6011097307731.\n"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Meilleurs hyperparamètres: {'fit_intercept': True, 'positive': False}\n","MSE optimisé de LinearRegression: 60.6011097307731\n","R2 correspondant: 0.39880656102690026\n","\n","                  0         1          2         3         4\n","Variable   Country  Latitude  Longitude      Year     Month\n","Poids     0.037473 -0.448113  -0.010225  0.002833  0.300089\n"]}],"source":["# Fonction objectif pour Optuna utilisant la Mean Squared Error (MSE)\n","def objective(trial):\n","    fit_intercept = trial.suggest_categorical('fit_intercept', [True, False])\n","    positive = trial.suggest_categorical('positive', [True, False])\n","\n","    model = LinearRegression(fit_intercept=fit_intercept, positive=positive)\n","    model.fit(X_train, y_train)\n","\n","    y_pred = model.predict(X_test)\n","    \n","    mse = mean_squared_error(y_test, y_pred)\n","\n","    return mse\n","\n","# Création et exécution de l'étude Optuna\n","study_reglin = optuna.create_study(direction='minimize')\n","study_reglin.optimize(objective, n_trials=50)\n","\n","# Obtention des meilleurs paramètres et calcul du MSE et du R2 avec les paramètres optimisés\n","best_params = study_reglin.best_params\n","best_model = LinearRegression(**best_params)\n","best_model.fit(X_train, y_train)\n","y_pred_best = best_model.predict(X_test)\n","mse_reglin= mean_squared_error(y_test, y_pred_best)\n","r2_reglin = r2_score(y_test, y_pred_best)\n","\n","print('\\n')\n","print(f\"Meilleurs hyperparamètres: {best_params}\")\n","print(f\"MSE optimisé de LinearRegression: {mse_reglin}\")\n","print(f\"R2 correspondant: {r2_reglin}\")\n","affichage_poids(best_model)"]},{"cell_type":"markdown","metadata":{},"source":["# RandomForestRegressor (assez lent à tourner)"]},{"cell_type":"code","execution_count":46,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["[I 2024-02-12 22:59:53,188] A new study created in memory with name: no-name-1dbb3909-10f5-4f51-867c-aed48cdf1fde\n","[I 2024-02-12 23:00:04,816] Trial 0 finished with value: 25.459602140557195 and parameters: {'n_estimators': 60, 'max_depth': 4, 'min_samples_split': 19, 'min_samples_leaf': 5}. Best is trial 0 with value: 25.459602140557195.\n","[I 2024-02-12 23:00:21,402] Trial 1 finished with value: 47.15604035220384 and parameters: {'n_estimators': 75, 'max_depth': 2, 'min_samples_split': 12, 'min_samples_leaf': 9}. Best is trial 0 with value: 25.459602140557195.\n","[I 2024-02-12 23:01:42,457] Trial 2 finished with value: 2.7706132112965816 and parameters: {'n_estimators': 79, 'max_depth': 11, 'min_samples_split': 10, 'min_samples_leaf': 1}. Best is trial 2 with value: 2.7706132112965816.\n","[I 2024-02-12 23:02:33,825] Trial 3 finished with value: 1.2810219363003068 and parameters: {'n_estimators': 41, 'max_depth': 25, 'min_samples_split': 2, 'min_samples_leaf': 8}. Best is trial 3 with value: 1.2810219363003068.\n","[I 2024-02-12 23:03:00,049] Trial 4 finished with value: 1.2985775643156852 and parameters: {'n_estimators': 34, 'max_depth': 24, 'min_samples_split': 14, 'min_samples_leaf': 5}. Best is trial 3 with value: 1.2810219363003068.\n","[I 2024-02-12 23:03:19,703] Trial 5 finished with value: 4.013225084454111 and parameters: {'n_estimators': 55, 'max_depth': 10, 'min_samples_split': 7, 'min_samples_leaf': 5}. Best is trial 3 with value: 1.2810219363003068.\n","[I 2024-02-12 23:04:12,719] Trial 6 finished with value: 4.024656154397898 and parameters: {'n_estimators': 100, 'max_depth': 10, 'min_samples_split': 4, 'min_samples_leaf': 7}. Best is trial 3 with value: 1.2810219363003068.\n","[I 2024-02-12 23:04:49,106] Trial 7 finished with value: 1.306724581274577 and parameters: {'n_estimators': 31, 'max_depth': 28, 'min_samples_split': 13, 'min_samples_leaf': 1}. Best is trial 3 with value: 1.2810219363003068.\n","[I 2024-02-12 23:04:59,211] Trial 8 finished with value: 47.15581607253127 and parameters: {'n_estimators': 47, 'max_depth': 2, 'min_samples_split': 10, 'min_samples_leaf': 3}. Best is trial 3 with value: 1.2810219363003068.\n","[I 2024-02-12 23:05:13,696] Trial 9 finished with value: 47.155834171986974 and parameters: {'n_estimators': 65, 'max_depth': 2, 'min_samples_split': 16, 'min_samples_leaf': 7}. Best is trial 3 with value: 1.2810219363003068.\n","[I 2024-02-12 23:05:23,194] Trial 10 finished with value: 1.3063211644433828 and parameters: {'n_estimators': 11, 'max_depth': 17, 'min_samples_split': 2, 'min_samples_leaf': 10}. Best is trial 3 with value: 1.2810219363003068.\n","[I 2024-02-12 23:05:50,423] Trial 11 finished with value: 1.2838961791360681 and parameters: {'n_estimators': 33, 'max_depth': 31, 'min_samples_split': 16, 'min_samples_leaf': 7}. Best is trial 3 with value: 1.2810219363003068.\n","[I 2024-02-12 23:06:10,641] Trial 12 finished with value: 1.2921237977346725 and parameters: {'n_estimators': 24, 'max_depth': 18, 'min_samples_split': 19, 'min_samples_leaf': 8}. Best is trial 3 with value: 1.2810219363003068.\n","[I 2024-02-12 23:06:50,005] Trial 13 finished with value: 1.2857893225378245 and parameters: {'n_estimators': 45, 'max_depth': 32, 'min_samples_split': 7, 'min_samples_leaf': 7}. Best is trial 3 with value: 1.2810219363003068.\n","[I 2024-02-12 23:06:54,911] Trial 14 finished with value: 14.552230597402662 and parameters: {'n_estimators': 15, 'max_depth': 6, 'min_samples_split': 16, 'min_samples_leaf': 10}. Best is trial 3 with value: 1.2810219363003068.\n","[I 2024-02-12 23:07:25,405] Trial 15 finished with value: 1.2892850601915051 and parameters: {'n_estimators': 40, 'max_depth': 18, 'min_samples_split': 7, 'min_samples_leaf': 8}. Best is trial 3 with value: 1.2810219363003068.\n","[I 2024-02-12 23:07:33,392] Trial 16 finished with value: 25.46461546017312 and parameters: {'n_estimators': 19, 'max_depth': 4, 'min_samples_split': 17, 'min_samples_leaf': 3}. Best is trial 3 with value: 1.2810219363003068.\n","[I 2024-02-12 23:07:50,954] Trial 17 finished with value: 1.6838032206632103 and parameters: {'n_estimators': 29, 'max_depth': 13, 'min_samples_split': 2, 'min_samples_leaf': 6}. Best is trial 3 with value: 1.2810219363003068.\n","[I 2024-02-12 23:08:23,500] Trial 18 finished with value: 1.2724775926704235 and parameters: {'n_estimators': 47, 'max_depth': 23, 'min_samples_split': 5, 'min_samples_leaf': 9}. Best is trial 18 with value: 1.2724775926704235.\n","[I 2024-02-12 23:08:53,278] Trial 19 finished with value: 10.046480471997452 and parameters: {'n_estimators': 74, 'max_depth': 7, 'min_samples_split': 5, 'min_samples_leaf': 9}. Best is trial 18 with value: 1.2724775926704235.\n","[I 2024-02-12 23:09:19,783] Trial 20 finished with value: 1.2736210705439854 and parameters: {'n_estimators': 49, 'max_depth': 21, 'min_samples_split': 4, 'min_samples_leaf': 9}. Best is trial 18 with value: 1.2724775926704235.\n","[I 2024-02-12 23:09:40,828] Trial 21 finished with value: 1.274697106128202 and parameters: {'n_estimators': 50, 'max_depth': 22, 'min_samples_split': 4, 'min_samples_leaf': 9}. Best is trial 18 with value: 1.2724775926704235.\n","[I 2024-02-12 23:10:02,151] Trial 22 finished with value: 1.2754416095306085 and parameters: {'n_estimators': 51, 'max_depth': 20, 'min_samples_split': 5, 'min_samples_leaf': 9}. Best is trial 18 with value: 1.2724775926704235.\n","[I 2024-02-12 23:10:26,303] Trial 23 finished with value: 1.4751008045380554 and parameters: {'n_estimators': 64, 'max_depth': 14, 'min_samples_split': 4, 'min_samples_leaf': 10}. Best is trial 18 with value: 1.2724775926704235.\n","[I 2024-02-12 23:10:50,382] Trial 24 finished with value: 1.274211965513925 and parameters: {'n_estimators': 56, 'max_depth': 22, 'min_samples_split': 8, 'min_samples_leaf': 9}. Best is trial 18 with value: 1.2724775926704235.\n","[I 2024-02-12 23:11:13,297] Trial 25 finished with value: 1.379153250333305 and parameters: {'n_estimators': 59, 'max_depth': 15, 'min_samples_split': 9, 'min_samples_leaf': 10}. Best is trial 18 with value: 1.2724775926704235.\n","[I 2024-02-12 23:11:28,792] Trial 26 finished with value: 10.074145447442092 and parameters: {'n_estimators': 67, 'max_depth': 7, 'min_samples_split': 8, 'min_samples_leaf': 8}. Best is trial 18 with value: 1.2724775926704235.\n","[I 2024-02-12 23:12:10,455] Trial 27 finished with value: 1.293581309310473 and parameters: {'n_estimators': 87, 'max_depth': 23, 'min_samples_split': 6, 'min_samples_leaf': 6}. Best is trial 18 with value: 1.2724775926704235.\n","[I 2024-02-12 23:12:26,548] Trial 28 finished with value: 1.695792355488246 and parameters: {'n_estimators': 40, 'max_depth': 13, 'min_samples_split': 9, 'min_samples_leaf': 9}. Best is trial 18 with value: 1.2724775926704235.\n","[I 2024-02-12 23:12:33,321] Trial 29 finished with value: 33.13828621226277 and parameters: {'n_estimators': 56, 'max_depth': 3, 'min_samples_split': 3, 'min_samples_leaf': 4}. Best is trial 18 with value: 1.2724775926704235.\n","[I 2024-02-12 23:12:48,963] Trial 30 finished with value: 7.144846620773751 and parameters: {'n_estimators': 61, 'max_depth': 8, 'min_samples_split': 6, 'min_samples_leaf': 8}. Best is trial 18 with value: 1.2724775926704235.\n","[I 2024-02-12 23:13:11,429] Trial 31 finished with value: 1.2728214510293934 and parameters: {'n_estimators': 52, 'max_depth': 23, 'min_samples_split': 4, 'min_samples_leaf': 9}. Best is trial 18 with value: 1.2724775926704235.\n","[I 2024-02-12 23:13:34,978] Trial 32 finished with value: 1.2740175222539387 and parameters: {'n_estimators': 51, 'max_depth': 27, 'min_samples_split': 6, 'min_samples_leaf': 9}. Best is trial 18 with value: 1.2724775926704235.\n","[I 2024-02-12 23:13:56,216] Trial 33 finished with value: 1.2702237704393355 and parameters: {'n_estimators': 50, 'max_depth': 27, 'min_samples_split': 5, 'min_samples_leaf': 10}. Best is trial 33 with value: 1.2702237704393355.\n","[I 2024-02-12 23:14:13,372] Trial 34 finished with value: 1.321543758278188 and parameters: {'n_estimators': 43, 'max_depth': 16, 'min_samples_split': 3, 'min_samples_leaf': 10}. Best is trial 33 with value: 1.2702237704393355.\n","[I 2024-02-12 23:14:28,491] Trial 35 finished with value: 1.2689185791795312 and parameters: {'n_estimators': 36, 'max_depth': 27, 'min_samples_split': 5, 'min_samples_leaf': 10}. Best is trial 35 with value: 1.2689185791795312.\n","[I 2024-02-12 23:14:43,908] Trial 36 finished with value: 1.2692469710984882 and parameters: {'n_estimators': 36, 'max_depth': 26, 'min_samples_split': 11, 'min_samples_leaf': 10}. Best is trial 35 with value: 1.2689185791795312.\n","[I 2024-02-12 23:14:59,559] Trial 37 finished with value: 1.2705540684441237 and parameters: {'n_estimators': 37, 'max_depth': 27, 'min_samples_split': 12, 'min_samples_leaf': 10}. Best is trial 35 with value: 1.2689185791795312.\n","[I 2024-02-12 23:15:15,210] Trial 38 finished with value: 1.2694314633589903 and parameters: {'n_estimators': 36, 'max_depth': 27, 'min_samples_split': 11, 'min_samples_leaf': 10}. Best is trial 35 with value: 1.2689185791795312.\n","[I 2024-02-12 23:15:26,444] Trial 39 finished with value: 1.2729544991780648 and parameters: {'n_estimators': 26, 'max_depth': 31, 'min_samples_split': 11, 'min_samples_leaf': 10}. Best is trial 35 with value: 1.2689185791795312.\n","[I 2024-02-12 23:15:37,689] Trial 40 finished with value: 2.830420610771161 and parameters: {'n_estimators': 36, 'max_depth': 11, 'min_samples_split': 14, 'min_samples_leaf': 10}. Best is trial 35 with value: 1.2689185791795312.\n","[I 2024-02-12 23:15:53,057] Trial 41 finished with value: 1.271738434800759 and parameters: {'n_estimators': 37, 'max_depth': 27, 'min_samples_split': 12, 'min_samples_leaf': 10}. Best is trial 35 with value: 1.2689185791795312.\n","[I 2024-02-12 23:16:02,744] Trial 42 finished with value: 1.2726635003232574 and parameters: {'n_estimators': 23, 'max_depth': 26, 'min_samples_split': 13, 'min_samples_leaf': 10}. Best is trial 35 with value: 1.2689185791795312.\n","[I 2024-02-12 23:16:16,093] Trial 43 finished with value: 1.2842647984581192 and parameters: {'n_estimators': 32, 'max_depth': 19, 'min_samples_split': 11, 'min_samples_leaf': 8}. Best is trial 35 with value: 1.2689185791795312.\n","[I 2024-02-12 23:16:32,171] Trial 44 finished with value: 1.2691903493393797 and parameters: {'n_estimators': 38, 'max_depth': 28, 'min_samples_split': 14, 'min_samples_leaf': 10}. Best is trial 35 with value: 1.2689185791795312.\n","[I 2024-02-12 23:16:44,646] Trial 45 finished with value: 1.2722113742075671 and parameters: {'n_estimators': 29, 'max_depth': 31, 'min_samples_split': 10, 'min_samples_leaf': 10}. Best is trial 35 with value: 1.2689185791795312.\n","[I 2024-02-12 23:17:02,571] Trial 46 finished with value: 1.2783323528130734 and parameters: {'n_estimators': 42, 'max_depth': 26, 'min_samples_split': 14, 'min_samples_leaf': 8}. Best is trial 35 with value: 1.2689185791795312.\n","[I 2024-02-12 23:17:10,879] Trial 47 finished with value: 1.2735607327655925 and parameters: {'n_estimators': 20, 'max_depth': 32, 'min_samples_split': 20, 'min_samples_leaf': 10}. Best is trial 35 with value: 1.2689185791795312.\n","[I 2024-02-12 23:17:24,535] Trial 48 finished with value: 1.2993586276275912 and parameters: {'n_estimators': 34, 'max_depth': 17, 'min_samples_split': 13, 'min_samples_leaf': 9}. Best is trial 35 with value: 1.2689185791795312.\n","[I 2024-02-12 23:17:44,805] Trial 49 finished with value: 1.2910429463226967 and parameters: {'n_estimators': 45, 'max_depth': 20, 'min_samples_split': 15, 'min_samples_leaf': 2}. Best is trial 35 with value: 1.2689185791795312.\n","[I 2024-02-12 23:17:48,837] Trial 50 finished with value: 25.464469463759652 and parameters: {'n_estimators': 27, 'max_depth': 4, 'min_samples_split': 18, 'min_samples_leaf': 7}. Best is trial 35 with value: 1.2689185791795312.\n","[I 2024-02-12 23:18:05,382] Trial 51 finished with value: 1.2702788278622072 and parameters: {'n_estimators': 39, 'max_depth': 29, 'min_samples_split': 12, 'min_samples_leaf': 10}. Best is trial 35 with value: 1.2689185791795312.\n","[I 2024-02-12 23:18:22,831] Trial 52 finished with value: 1.271001828260501 and parameters: {'n_estimators': 41, 'max_depth': 29, 'min_samples_split': 14, 'min_samples_leaf': 10}. Best is trial 35 with value: 1.2689185791795312.\n","[I 2024-02-12 23:18:38,549] Trial 53 finished with value: 1.2700735790584912 and parameters: {'n_estimators': 37, 'max_depth': 24, 'min_samples_split': 10, 'min_samples_leaf': 10}. Best is trial 35 with value: 1.2689185791795312.\n","[I 2024-02-12 23:18:52,807] Trial 54 finished with value: 1.2756873031077531 and parameters: {'n_estimators': 32, 'max_depth': 25, 'min_samples_split': 10, 'min_samples_leaf': 9}. Best is trial 35 with value: 1.2689185791795312.\n","[I 2024-02-12 23:19:00,895] Trial 55 finished with value: 19.115364594193096 and parameters: {'n_estimators': 46, 'max_depth': 5, 'min_samples_split': 9, 'min_samples_leaf': 9}. Best is trial 35 with value: 1.2689185791795312.\n","[I 2024-02-12 23:19:14,888] Trial 56 finished with value: 1.2741596381853737 and parameters: {'n_estimators': 35, 'max_depth': 19, 'min_samples_split': 11, 'min_samples_leaf': 10}. Best is trial 35 with value: 1.2689185791795312.\n","[I 2024-02-12 23:19:23,625] Trial 57 finished with value: 1.2738375051219948 and parameters: {'n_estimators': 22, 'max_depth': 24, 'min_samples_split': 8, 'min_samples_leaf': 10}. Best is trial 35 with value: 1.2689185791795312.\n","[I 2024-02-12 23:19:29,734] Trial 58 finished with value: 1.3050104839868983 and parameters: {'n_estimators': 15, 'max_depth': 21, 'min_samples_split': 15, 'min_samples_leaf': 4}. Best is trial 35 with value: 1.2689185791795312.\n","[I 2024-02-12 23:19:38,341] Trial 59 finished with value: 2.773751702686639 and parameters: {'n_estimators': 29, 'max_depth': 11, 'min_samples_split': 12, 'min_samples_leaf': 9}. Best is trial 35 with value: 1.2689185791795312.\n","[I 2024-02-12 23:19:52,859] Trial 60 finished with value: 1.300468646205488 and parameters: {'n_estimators': 38, 'max_depth': 17, 'min_samples_split': 7, 'min_samples_leaf': 9}. Best is trial 35 with value: 1.2689185791795312.\n","[I 2024-02-12 23:20:09,914] Trial 61 finished with value: 1.270963268218389 and parameters: {'n_estimators': 44, 'max_depth': 28, 'min_samples_split': 10, 'min_samples_leaf': 10}. Best is trial 35 with value: 1.2689185791795312.\n","[I 2024-02-12 23:20:25,594] Trial 62 finished with value: 1.2719243677146583 and parameters: {'n_estimators': 40, 'max_depth': 29, 'min_samples_split': 12, 'min_samples_leaf': 10}. Best is trial 35 with value: 1.2689185791795312.\n","[I 2024-02-12 23:20:44,354] Trial 63 finished with value: 1.2704984569886524 and parameters: {'n_estimators': 48, 'max_depth': 25, 'min_samples_split': 13, 'min_samples_leaf': 10}. Best is trial 35 with value: 1.2689185791795312.\n","[I 2024-02-12 23:20:56,628] Trial 64 finished with value: 1.2760580219257736 and parameters: {'n_estimators': 31, 'max_depth': 32, 'min_samples_split': 15, 'min_samples_leaf': 9}. Best is trial 35 with value: 1.2689185791795312.\n","[I 2024-02-12 23:21:17,974] Trial 65 finished with value: 1.2703485941641723 and parameters: {'n_estimators': 54, 'max_depth': 22, 'min_samples_split': 10, 'min_samples_leaf': 10}. Best is trial 35 with value: 1.2689185791795312.\n","[I 2024-02-12 23:21:33,141] Trial 66 finished with value: 1.2820991544908822 and parameters: {'n_estimators': 38, 'max_depth': 29, 'min_samples_split': 13, 'min_samples_leaf': 8}. Best is trial 35 with value: 1.2689185791795312.\n","[I 2024-02-12 23:21:42,929] Trial 67 finished with value: 33.138217285504226 and parameters: {'n_estimators': 92, 'max_depth': 3, 'min_samples_split': 11, 'min_samples_leaf': 6}. Best is trial 35 with value: 1.2689185791795312.\n","[I 2024-02-12 23:21:56,225] Trial 68 finished with value: 1.277615106395583 and parameters: {'n_estimators': 34, 'max_depth': 24, 'min_samples_split': 11, 'min_samples_leaf': 9}. Best is trial 35 with value: 1.2689185791795312.\n","[I 2024-02-12 23:22:06,255] Trial 69 finished with value: 5.18457994335031 and parameters: {'n_estimators': 39, 'max_depth': 9, 'min_samples_split': 9, 'min_samples_leaf': 10}. Best is trial 35 with value: 1.2689185791795312.\n","[I 2024-02-12 23:22:16,521] Trial 70 finished with value: 1.3833432492367264 and parameters: {'n_estimators': 26, 'max_depth': 15, 'min_samples_split': 17, 'min_samples_leaf': 10}. Best is trial 35 with value: 1.2689185791795312.\n","[I 2024-02-12 23:22:44,941] Trial 71 finished with value: 1.2691290817619152 and parameters: {'n_estimators': 59, 'max_depth': 22, 'min_samples_split': 10, 'min_samples_leaf': 10}. Best is trial 35 with value: 1.2689185791795312.\n","[I 2024-02-12 23:23:16,581] Trial 72 finished with value: 1.2698082958640975 and parameters: {'n_estimators': 71, 'max_depth': 21, 'min_samples_split': 8, 'min_samples_leaf': 10}. Best is trial 35 with value: 1.2689185791795312.\n","[I 2024-02-12 23:23:44,665] Trial 73 finished with value: 1.268543884430763 and parameters: {'n_estimators': 68, 'max_depth': 21, 'min_samples_split': 8, 'min_samples_leaf': 10}. Best is trial 73 with value: 1.268543884430763.\n","[I 2024-02-12 23:24:13,385] Trial 74 finished with value: 1.2742172800364855 and parameters: {'n_estimators': 69, 'max_depth': 20, 'min_samples_split': 8, 'min_samples_leaf': 9}. Best is trial 73 with value: 1.268543884430763.\n","[I 2024-02-12 23:24:41,587] Trial 75 finished with value: 1.279709824740005 and parameters: {'n_estimators': 74, 'max_depth': 18, 'min_samples_split': 9, 'min_samples_leaf': 10}. Best is trial 73 with value: 1.268543884430763.\n","[I 2024-02-12 23:25:12,741] Trial 76 finished with value: 1.2739827692319956 and parameters: {'n_estimators': 78, 'max_depth': 22, 'min_samples_split': 7, 'min_samples_leaf': 9}. Best is trial 73 with value: 1.268543884430763.\n","[I 2024-02-12 23:25:36,159] Trial 77 finished with value: 1.6950435052890431 and parameters: {'n_estimators': 71, 'max_depth': 13, 'min_samples_split': 8, 'min_samples_leaf': 10}. Best is trial 73 with value: 1.268543884430763.\n","[I 2024-02-12 23:26:05,904] Trial 78 finished with value: 1.3180526855150636 and parameters: {'n_estimators': 61, 'max_depth': 21, 'min_samples_split': 9, 'min_samples_leaf': 1}. Best is trial 73 with value: 1.268543884430763.\n","[I 2024-02-12 23:26:29,233] Trial 79 finished with value: 1.2745414758207978 and parameters: {'n_estimators': 59, 'max_depth': 24, 'min_samples_split': 10, 'min_samples_leaf': 9}. Best is trial 73 with value: 1.268543884430763.\n","[I 2024-02-12 23:26:57,755] Trial 80 finished with value: 1.323199967789031 and parameters: {'n_estimators': 78, 'max_depth': 16, 'min_samples_split': 11, 'min_samples_leaf': 10}. Best is trial 73 with value: 1.268543884430763.\n","[I 2024-02-12 23:27:23,537] Trial 81 finished with value: 1.2685747703093104 and parameters: {'n_estimators': 66, 'max_depth': 26, 'min_samples_split': 5, 'min_samples_leaf': 10}. Best is trial 73 with value: 1.268543884430763.\n","[I 2024-02-12 23:27:48,427] Trial 82 finished with value: 1.2688778459047305 and parameters: {'n_estimators': 64, 'max_depth': 23, 'min_samples_split': 6, 'min_samples_leaf': 10}. Best is trial 73 with value: 1.268543884430763.\n","[I 2024-02-12 23:28:13,025] Trial 83 finished with value: 1.2725199956487423 and parameters: {'n_estimators': 64, 'max_depth': 19, 'min_samples_split': 6, 'min_samples_leaf': 10}. Best is trial 73 with value: 1.268543884430763.\n","[I 2024-02-12 23:28:41,475] Trial 84 finished with value: 1.269949386350351 and parameters: {'n_estimators': 71, 'max_depth': 22, 'min_samples_split': 5, 'min_samples_leaf': 10}. Best is trial 73 with value: 1.268543884430763.\n","[I 2024-02-12 23:29:08,118] Trial 85 finished with value: 1.273240342601303 and parameters: {'n_estimators': 66, 'max_depth': 26, 'min_samples_split': 6, 'min_samples_leaf': 9}. Best is trial 73 with value: 1.268543884430763.\n","[I 2024-02-12 23:29:32,524] Trial 86 finished with value: 1.2671129043548772 and parameters: {'n_estimators': 63, 'max_depth': 29, 'min_samples_split': 5, 'min_samples_leaf': 10}. Best is trial 86 with value: 1.2671129043548772.\n","[I 2024-02-12 23:29:56,898] Trial 87 finished with value: 1.2683113287342513 and parameters: {'n_estimators': 63, 'max_depth': 30, 'min_samples_split': 3, 'min_samples_leaf': 10}. Best is trial 86 with value: 1.2671129043548772.\n","[I 2024-02-12 23:30:22,419] Trial 88 finished with value: 1.2711763055097598 and parameters: {'n_estimators': 64, 'max_depth': 30, 'min_samples_split': 3, 'min_samples_leaf': 9}. Best is trial 86 with value: 1.2671129043548772.\n","[I 2024-02-12 23:30:45,480] Trial 89 finished with value: 1.2692259925136649 and parameters: {'n_estimators': 59, 'max_depth': 32, 'min_samples_split': 4, 'min_samples_leaf': 10}. Best is trial 86 with value: 1.2671129043548772.\n","[I 2024-02-12 23:31:08,324] Trial 90 finished with value: 1.2705033621289812 and parameters: {'n_estimators': 59, 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 10}. Best is trial 86 with value: 1.2671129043548772.\n","[I 2024-02-12 23:31:32,509] Trial 91 finished with value: 1.268968325524487 and parameters: {'n_estimators': 62, 'max_depth': 32, 'min_samples_split': 4, 'min_samples_leaf': 10}. Best is trial 86 with value: 1.2671129043548772.\n","[I 2024-02-12 23:31:54,464] Trial 92 finished with value: 1.2690029063043184 and parameters: {'n_estimators': 57, 'max_depth': 32, 'min_samples_split': 3, 'min_samples_leaf': 10}. Best is trial 86 with value: 1.2671129043548772.\n","[I 2024-02-12 23:32:18,632] Trial 93 finished with value: 1.2690365316051933 and parameters: {'n_estimators': 62, 'max_depth': 28, 'min_samples_split': 3, 'min_samples_leaf': 10}. Best is trial 86 with value: 1.2671129043548772.\n","[I 2024-02-12 23:32:43,164] Trial 94 finished with value: 1.2722954621663847 and parameters: {'n_estimators': 62, 'max_depth': 25, 'min_samples_split': 3, 'min_samples_leaf': 9}. Best is trial 86 with value: 1.2671129043548772.\n","[I 2024-02-12 23:33:09,704] Trial 95 finished with value: 1.269129564835331 and parameters: {'n_estimators': 68, 'max_depth': 28, 'min_samples_split': 5, 'min_samples_leaf': 10}. Best is trial 86 with value: 1.2671129043548772.\n","[I 2024-02-12 23:33:31,723] Trial 96 finished with value: 1.269055977232797 and parameters: {'n_estimators': 57, 'max_depth': 32, 'min_samples_split': 3, 'min_samples_leaf': 10}. Best is trial 86 with value: 1.2671129043548772.\n","[I 2024-02-12 23:33:53,460] Trial 97 finished with value: 1.2734977445432147 and parameters: {'n_estimators': 55, 'max_depth': 32, 'min_samples_split': 2, 'min_samples_leaf': 9}. Best is trial 86 with value: 1.2671129043548772.\n","[I 2024-02-12 23:34:31,478] Trial 98 finished with value: 1.3955458028103123 and parameters: {'n_estimators': 62, 'max_depth': 30, 'min_samples_split': 4, 'min_samples_leaf': 2}. Best is trial 86 with value: 1.2671129043548772.\n","[I 2024-02-12 23:34:53,817] Trial 99 finished with value: 1.2682391410132399 and parameters: {'n_estimators': 57, 'max_depth': 27, 'min_samples_split': 3, 'min_samples_leaf': 10}. Best is trial 86 with value: 1.2671129043548772.\n"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Meilleurs hyperparamètres:\n","n_estimators: 60\n","max_depth: 4\n","min_samples_split: 19\n","min_samples_leaf: 5\n","Meilleure MSE: 1.2671\n","R2 correspondant: 0.9874\n","\n"," Importance des variables :\n","                 1         4          2         0         3\n","Variable  Latitude     Month  Longitude   Country      Year\n","Poids     0.532454  0.328871    0.08618  0.048255  0.004241\n"]}],"source":["# On initialise avec aucune valeur d'hyperparamètres\n","r2_scores_rf = {'n_estimators': [], 'max_depth': [], 'min_samples_split': [], 'min_samples_leaf': []}\n","mse_scores_rf = {'n_estimators': [], 'max_depth': [], 'min_samples_split': [], 'min_samples_leaf': []}\n","hyperparameters = {'n_estimators': [], 'max_depth': [], 'min_samples_split': [], 'min_samples_leaf': []}\n","\n","def objective_rf(trial):\n","    params = {\n","        'n_estimators': trial.suggest_int('n_estimators', 10, 100),\n","        'max_depth': trial.suggest_int('max_depth', 2, 32, log=True),\n","        'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n","        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n","    }\n","    \n","    rf = RandomForestRegressor(**params)\n","    rf.fit(X_train, y_train)\n","    y_pred = rf.predict(X_test)\n","    \n","    r2 = r2_score(y_test, y_pred)\n","    mse = mean_squared_error(y_test, y_pred)\n","    \n","    # Stocke les scores R2 et MSE pour chaque hyperparamètre\n","    for param_name, param_value in params.items():\n","        r2_scores_rf[param_name].append(r2)\n","        hyperparameters[param_name].append(param_value)\n","        mse_scores_rf[param_name].append(mse)\n","    \n","    return mse  # Optuna cherche à minimiser, donc utilise la MSE\n","\n","\n","# Crée une étude Optuna pour RandomForestRegressor\n","study_rf = optuna.create_study(direction='minimize')\n","study_rf.optimize(objective_rf, n_trials=100)\n","\n","# Obtient l'indice du meilleur score MSE à travers tous les hyperparamètres\n","best_mse_index = np.argmin([min(mses) for mses in mse_scores_rf.values()])\n","\n","# Extrait les meilleurs hyperparamètres\n","best_hyperparameters = {param_name: values[best_mse_index] for param_name, values in hyperparameters.items()}\n","\n","print('\\n')\n","print(\"Meilleurs hyperparamètres:\")\n","for param_name, param_value in best_hyperparameters.items():\n","    print(f\"{param_name}: {param_value}\")\n","\n","best_mse_RandomForest = min([min(mses) for mses in mse_scores_rf.values()])\n","r2_RandomForest = max([max(scores) for scores in r2_scores_rf.values()])\n","print(f\"Meilleure MSE: {best_mse_RandomForest:.4f}\")\n","print(f\"R2 correspondant: {r2_RandomForest:.4f}\")\n","\n","#Affichage des poids\n","best_RandomForest= RandomForestRegressor(**study_rf.best_params)\n","best_RandomForest.fit(X_train, y_train)\n","\n","# Obtention du poids des variables\n","feature_importance = best_RandomForest.feature_importances_\n","\n","# Création d'un DataFrame pour afficher les importances des variables\n","importance_df = pd.DataFrame({'Variable': X_train.columns, 'Poids': feature_importance})\n","importance_df = importance_df.sort_values('Poids', ascending=False)\n","\n","print(\"\\n Importance des variables :\")\n","importance=importance_df.T\n","print(importance.to_string())\n","\n","\n","with open('study_rf.pkl', 'wb') as f:\n","    pickle.dump(study_rf, f)\n","    \n","with open('study_rf.pkl', 'rb') as f:\n","    study_rf = pickle.load(f)"]},{"cell_type":"markdown","metadata":{},"source":["# Gradient Boosting Regressor (assez lent lui aussi)"]},{"cell_type":"code","execution_count":47,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["[I 2024-02-12 23:35:17,380] A new study created in memory with name: no-name-c3ef6d17-04ff-47ac-8fb6-fddf8a9ee5f2\n","[I 2024-02-12 23:35:25,119] Trial 0 finished with value: 2.9071568898723125e+95 and parameters: {'n_estimators': 26, 'learning_rate': 64, 'min_weight_fraction_leaf': 0.02287420084632552, 'max_depth': 20, 'min_impurity_decrease': 80, 'min_samples_leaf': 2}. Best is trial 0 with value: 2.9071568898723125e+95.\n","C:\\Users\\valentin\\AppData\\Roaming\\Python\\Python311\\site-packages\\numpy\\core\\_methods.py:49: RuntimeWarning: overflow encountered in reduce\n","  return umr_sum(a, axis, dtype, out, keepdims, initial, where)\n","C:\\Users\\valentin\\AppData\\Roaming\\Python\\Python311\\site-packages\\numpy\\core\\_methods.py:49: RuntimeWarning: overflow encountered in reduce\n","  return umr_sum(a, axis, dtype, out, keepdims, initial, where)\n","C:\\Users\\valentin\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\metrics\\_regression.py:501: RuntimeWarning: overflow encountered in square\n","  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)\n","[I 2024-02-12 23:35:31,196] Trial 1 finished with value: inf and parameters: {'n_estimators': 59, 'learning_rate': 497, 'min_weight_fraction_leaf': 0.08865765362192496, 'max_depth': 2, 'min_impurity_decrease': 524, 'min_samples_leaf': 6}. Best is trial 0 with value: 2.9071568898723125e+95.\n","C:\\Users\\valentin\\AppData\\Roaming\\Python\\Python311\\site-packages\\numpy\\core\\_methods.py:49: RuntimeWarning: overflow encountered in reduce\n","  return umr_sum(a, axis, dtype, out, keepdims, initial, where)\n","C:\\Users\\valentin\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\metrics\\_regression.py:501: RuntimeWarning: overflow encountered in square\n","  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)\n","[I 2024-02-12 23:35:36,030] Trial 2 finished with value: inf and parameters: {'n_estimators': 82, 'learning_rate': 988, 'min_weight_fraction_leaf': 0.439667562182107, 'max_depth': 10, 'min_impurity_decrease': 371, 'min_samples_leaf': 5}. Best is trial 0 with value: 2.9071568898723125e+95.\n","C:\\Users\\valentin\\AppData\\Roaming\\Python\\Python311\\site-packages\\numpy\\core\\_methods.py:49: RuntimeWarning: overflow encountered in reduce\n","  return umr_sum(a, axis, dtype, out, keepdims, initial, where)\n","C:\\Users\\valentin\\AppData\\Roaming\\Python\\Python311\\site-packages\\numpy\\core\\_methods.py:49: RuntimeWarning: overflow encountered in reduce\n","  return umr_sum(a, axis, dtype, out, keepdims, initial, where)\n","C:\\Users\\valentin\\AppData\\Roaming\\Python\\Python311\\site-packages\\numpy\\core\\_methods.py:49: RuntimeWarning: overflow encountered in reduce\n","  return umr_sum(a, axis, dtype, out, keepdims, initial, where)\n","C:\\Users\\valentin\\AppData\\Roaming\\Python\\Python311\\site-packages\\numpy\\core\\_methods.py:49: RuntimeWarning: overflow encountered in reduce\n","  return umr_sum(a, axis, dtype, out, keepdims, initial, where)\n","C:\\Users\\valentin\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\metrics\\_regression.py:501: RuntimeWarning: overflow encountered in square\n","  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)\n","[I 2024-02-12 23:35:40,982] Trial 3 finished with value: inf and parameters: {'n_estimators': 82, 'learning_rate': 177, 'min_weight_fraction_leaf': 0.3382981971367564, 'max_depth': 20, 'min_impurity_decrease': 807, 'min_samples_leaf': 4}. Best is trial 0 with value: 2.9071568898723125e+95.\n","C:\\Users\\valentin\\AppData\\Roaming\\Python\\Python311\\site-packages\\numpy\\core\\_methods.py:49: RuntimeWarning: overflow encountered in reduce\n","  return umr_sum(a, axis, dtype, out, keepdims, initial, where)\n","C:\\Users\\valentin\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\metrics\\_regression.py:501: RuntimeWarning: overflow encountered in square\n","  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)\n","[I 2024-02-12 23:35:49,592] Trial 4 finished with value: inf and parameters: {'n_estimators': 88, 'learning_rate': 512, 'min_weight_fraction_leaf': 0.22756132636456333, 'max_depth': 8, 'min_impurity_decrease': 875, 'min_samples_leaf': 9}. Best is trial 0 with value: 2.9071568898723125e+95.\n","[I 2024-02-12 23:35:51,937] Trial 5 finished with value: 8.087576939411265e+235 and parameters: {'n_estimators': 40, 'learning_rate': 850, 'min_weight_fraction_leaf': 0.300257646473608, 'max_depth': 2, 'min_impurity_decrease': 974, 'min_samples_leaf': 1}. Best is trial 0 with value: 2.9071568898723125e+95.\n","[I 2024-02-12 23:35:55,004] Trial 6 finished with value: 1.281854975470392e+155 and parameters: {'n_estimators': 28, 'learning_rate': 551, 'min_weight_fraction_leaf': 0.195896705392616, 'max_depth': 26, 'min_impurity_decrease': 905, 'min_samples_leaf': 6}. Best is trial 0 with value: 2.9071568898723125e+95.\n","[I 2024-02-12 23:35:55,769] Trial 7 finished with value: 5.2574433030265734e+72 and parameters: {'n_estimators': 13, 'learning_rate': 545, 'min_weight_fraction_leaf': 0.31753060006255335, 'max_depth': 2, 'min_impurity_decrease': 267, 'min_samples_leaf': 9}. Best is trial 7 with value: 5.2574433030265734e+72.\n","[I 2024-02-12 23:35:58,309] Trial 8 finished with value: 2.486817479245979e+223 and parameters: {'n_estimators': 42, 'learning_rate': 438, 'min_weight_fraction_leaf': 0.3659645269736127, 'max_depth': 25, 'min_impurity_decrease': 4, 'min_samples_leaf': 8}. Best is trial 7 with value: 5.2574433030265734e+72.\n","C:\\Users\\valentin\\AppData\\Roaming\\Python\\Python311\\site-packages\\numpy\\core\\_methods.py:49: RuntimeWarning: overflow encountered in reduce\n","  return umr_sum(a, axis, dtype, out, keepdims, initial, where)\n","C:\\Users\\valentin\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\metrics\\_regression.py:501: RuntimeWarning: overflow encountered in square\n","  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)\n","[I 2024-02-12 23:36:03,105] Trial 9 finished with value: inf and parameters: {'n_estimators': 81, 'learning_rate': 426, 'min_weight_fraction_leaf': 0.46032645698482433, 'max_depth': 3, 'min_impurity_decrease': 822, 'min_samples_leaf': 6}. Best is trial 7 with value: 5.2574433030265734e+72.\n","[I 2024-02-12 23:36:04,979] Trial 10 finished with value: 8.930379126541264e+87 and parameters: {'n_estimators': 15, 'learning_rate': 752, 'min_weight_fraction_leaf': 0.15030988785517962, 'max_depth': 4, 'min_impurity_decrease': 298, 'min_samples_leaf': 10}. Best is trial 7 with value: 5.2574433030265734e+72.\n","[I 2024-02-12 23:36:06,361] Trial 11 finished with value: 5.0224493973207477e+64 and parameters: {'n_estimators': 11, 'learning_rate': 733, 'min_weight_fraction_leaf': 0.1434189516731159, 'max_depth': 4, 'min_impurity_decrease': 305, 'min_samples_leaf': 10}. Best is trial 11 with value: 5.0224493973207477e+64.\n","[I 2024-02-12 23:36:08,054] Trial 12 finished with value: 7.732897744815819e+63 and parameters: {'n_estimators': 11, 'learning_rate': 667, 'min_weight_fraction_leaf': 0.11961351199241893, 'max_depth': 5, 'min_impurity_decrease': 207, 'min_samples_leaf': 8}. Best is trial 12 with value: 7.732897744815819e+63.\n","[I 2024-02-12 23:36:09,689] Trial 13 finished with value: 2.1421420093666543e+59 and parameters: {'n_estimators': 10, 'learning_rate': 756, 'min_weight_fraction_leaf': 0.11850188994223279, 'max_depth': 5, 'min_impurity_decrease': 516, 'min_samples_leaf': 8}. Best is trial 13 with value: 2.1421420093666543e+59.\n","C:\\Users\\valentin\\AppData\\Roaming\\Python\\Python311\\site-packages\\numpy\\core\\_methods.py:49: RuntimeWarning: overflow encountered in reduce\n","  return umr_sum(a, axis, dtype, out, keepdims, initial, where)\n","C:\\Users\\valentin\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\metrics\\_regression.py:501: RuntimeWarning: overflow encountered in square\n","  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)\n","[I 2024-02-12 23:36:25,627] Trial 14 finished with value: inf and parameters: {'n_estimators': 58, 'learning_rate': 717, 'min_weight_fraction_leaf': 0.0027879964821644065, 'max_depth': 6, 'min_impurity_decrease': 613, 'min_samples_leaf': 8}. Best is trial 13 with value: 2.1421420093666543e+59.\n","[I 2024-02-12 23:36:30,562] Trial 15 finished with value: 1.0568474422074645e+168 and parameters: {'n_estimators': 28, 'learning_rate': 930, 'min_weight_fraction_leaf': 0.08012439611114398, 'max_depth': 12, 'min_impurity_decrease': 610, 'min_samples_leaf': 7}. Best is trial 13 with value: 2.1421420093666543e+59.\n","[I 2024-02-12 23:36:38,446] Trial 16 finished with value: 5.721571603104439e+248 and parameters: {'n_estimators': 44, 'learning_rate': 641, 'min_weight_fraction_leaf': 0.09247578410241009, 'max_depth': 6, 'min_impurity_decrease': 171, 'min_samples_leaf': 4}. Best is trial 13 with value: 2.1421420093666543e+59.\n","[I 2024-02-12 23:36:41,325] Trial 17 finished with value: 2.083833077853421e+106 and parameters: {'n_estimators': 21, 'learning_rate': 311, 'min_weight_fraction_leaf': 0.15883179843851497, 'max_depth': 4, 'min_impurity_decrease': 441, 'min_samples_leaf': 8}. Best is trial 13 with value: 2.1421420093666543e+59.\n","C:\\Users\\valentin\\AppData\\Roaming\\Python\\Python311\\site-packages\\numpy\\core\\_methods.py:49: RuntimeWarning: overflow encountered in reduce\n","  return umr_sum(a, axis, dtype, out, keepdims, initial, where)\n","C:\\Users\\valentin\\AppData\\Roaming\\Python\\Python311\\site-packages\\numpy\\core\\_methods.py:49: RuntimeWarning: overflow encountered in reduce\n","  return umr_sum(a, axis, dtype, out, keepdims, initial, where)\n","C:\\Users\\valentin\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\metrics\\_regression.py:501: RuntimeWarning: overflow encountered in square\n","  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)\n","[I 2024-02-12 23:36:50,243] Trial 18 finished with value: inf and parameters: {'n_estimators': 100, 'learning_rate': 813, 'min_weight_fraction_leaf': 0.25464220546214666, 'max_depth': 3, 'min_impurity_decrease': 692, 'min_samples_leaf': 7}. Best is trial 13 with value: 2.1421420093666543e+59.\n","C:\\Users\\valentin\\AppData\\Roaming\\Python\\Python311\\site-packages\\numpy\\core\\_methods.py:49: RuntimeWarning: overflow encountered in reduce\n","  return umr_sum(a, axis, dtype, out, keepdims, initial, where)\n","C:\\Users\\valentin\\AppData\\Roaming\\Python\\Python311\\site-packages\\numpy\\core\\_methods.py:49: RuntimeWarning: overflow encountered in reduce\n","  return umr_sum(a, axis, dtype, out, keepdims, initial, where)\n","C:\\Users\\valentin\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\metrics\\_regression.py:501: RuntimeWarning: overflow encountered in square\n","  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)\n","[I 2024-02-12 23:37:06,256] Trial 19 finished with value: inf and parameters: {'n_estimators': 68, 'learning_rate': 642, 'min_weight_fraction_leaf': 0.051835086907341704, 'max_depth': 8, 'min_impurity_decrease': 200, 'min_samples_leaf': 9}. Best is trial 13 with value: 2.1421420093666543e+59.\n","[I 2024-02-12 23:37:11,460] Trial 20 finished with value: 1.3936936023080903e+226 and parameters: {'n_estimators': 38, 'learning_rate': 897, 'min_weight_fraction_leaf': 0.12659873371500258, 'max_depth': 5, 'min_impurity_decrease': 474, 'min_samples_leaf': 7}. Best is trial 13 with value: 2.1421420093666543e+59.\n","[I 2024-02-12 23:37:12,766] Trial 21 finished with value: 3.554829943286074e+69 and parameters: {'n_estimators': 12, 'learning_rate': 676, 'min_weight_fraction_leaf': 0.19508222387161778, 'max_depth': 3, 'min_impurity_decrease': 343, 'min_samples_leaf': 10}. Best is trial 13 with value: 2.1421420093666543e+59.\n","[I 2024-02-12 23:37:14,229] Trial 22 finished with value: 2.9921473220741717e+59 and parameters: {'n_estimators': 10, 'learning_rate': 769, 'min_weight_fraction_leaf': 0.12925030392008158, 'max_depth': 4, 'min_impurity_decrease': 184, 'min_samples_leaf': 10}. Best is trial 13 with value: 2.1421420093666543e+59.\n","[I 2024-02-12 23:37:17,310] Trial 23 finished with value: 7.112542584740358e+117 and parameters: {'n_estimators': 20, 'learning_rate': 799, 'min_weight_fraction_leaf': 0.11395958231816716, 'max_depth': 12, 'min_impurity_decrease': 113, 'min_samples_leaf': 9}. Best is trial 13 with value: 2.1421420093666543e+59.\n","[I 2024-02-12 23:37:21,656] Trial 24 finished with value: 7.191016074257264e+118 and parameters: {'n_estimators': 21, 'learning_rate': 612, 'min_weight_fraction_leaf': 0.05138597134159753, 'max_depth': 5, 'min_impurity_decrease': 212, 'min_samples_leaf': 8}. Best is trial 13 with value: 2.1421420093666543e+59.\n","[I 2024-02-12 23:37:25,258] Trial 25 finished with value: 1.744408309461266e+199 and parameters: {'n_estimators': 34, 'learning_rate': 806, 'min_weight_fraction_leaf': 0.194533345108467, 'max_depth': 3, 'min_impurity_decrease': 405, 'min_samples_leaf': 10}. Best is trial 13 with value: 2.1421420093666543e+59.\n","C:\\Users\\valentin\\AppData\\Roaming\\Python\\Python311\\site-packages\\numpy\\core\\_methods.py:49: RuntimeWarning: overflow encountered in reduce\n","  return umr_sum(a, axis, dtype, out, keepdims, initial, where)\n","C:\\Users\\valentin\\AppData\\Roaming\\Python\\Python311\\site-packages\\numpy\\core\\_methods.py:118: RuntimeWarning: overflow encountered in reduce\n","  ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)\n","[I 2024-02-12 23:37:29,666] Trial 26 finished with value: inf and parameters: {'n_estimators': 51, 'learning_rate': 993, 'min_weight_fraction_leaf': 0.2615416636183778, 'max_depth': 5, 'min_impurity_decrease': 19, 'min_samples_leaf': 9}. Best is trial 13 with value: 2.1421420093666543e+59.\n","[I 2024-02-12 23:37:31,963] Trial 27 finished with value: 7.212598010190763e+107 and parameters: {'n_estimators': 18, 'learning_rate': 891, 'min_weight_fraction_leaf': 0.17222720872770486, 'max_depth': 7, 'min_impurity_decrease': 545, 'min_samples_leaf': 7}. Best is trial 13 with value: 2.1421420093666543e+59.\n","[I 2024-02-12 23:37:34,403] Trial 28 finished with value: 1.8886544911010885e+53 and parameters: {'n_estimators': 10, 'learning_rate': 373, 'min_weight_fraction_leaf': 0.03751075652591651, 'max_depth': 10, 'min_impurity_decrease': 132, 'min_samples_leaf': 5}. Best is trial 28 with value: 1.8886544911010885e+53.\n","[I 2024-02-12 23:37:41,707] Trial 29 finished with value: 1.1821560692445012e+105 and parameters: {'n_estimators': 32, 'learning_rate': 42, 'min_weight_fraction_leaf': 0.045830479282548645, 'max_depth': 17, 'min_impurity_decrease': 84, 'min_samples_leaf': 4}. Best is trial 28 with value: 1.8886544911010885e+53.\n","[I 2024-02-12 23:37:51,950] Trial 30 finished with value: 1.5009497595911766e+125 and parameters: {'n_estimators': 25, 'learning_rate': 292, 'min_weight_fraction_leaf': 0.0024833098288959757, 'max_depth': 15, 'min_impurity_decrease': 136, 'min_samples_leaf': 3}. Best is trial 28 with value: 1.8886544911010885e+53.\n","[I 2024-02-12 23:37:53,506] Trial 31 finished with value: 1.7449996888701375e+52 and parameters: {'n_estimators': 10, 'learning_rate': 334, 'min_weight_fraction_leaf': 0.1068858523820304, 'max_depth': 9, 'min_impurity_decrease': 202, 'min_samples_leaf': 5}. Best is trial 31 with value: 1.7449996888701375e+52.\n","[I 2024-02-12 23:37:57,841] Trial 32 finished with value: 2.1333444279109426e+117 and parameters: {'n_estimators': 24, 'learning_rate': 256, 'min_weight_fraction_leaf': 0.08323770827823634, 'max_depth': 10, 'min_impurity_decrease': 255, 'min_samples_leaf': 5}. Best is trial 31 with value: 1.7449996888701375e+52.\n","[I 2024-02-12 23:38:01,138] Trial 33 finished with value: 4.036691483351678e+84 and parameters: {'n_estimators': 16, 'learning_rate': 387, 'min_weight_fraction_leaf': 0.06636533072669486, 'max_depth': 10, 'min_impurity_decrease': 57, 'min_samples_leaf': 5}. Best is trial 31 with value: 1.7449996888701375e+52.\n","[I 2024-02-12 23:38:03,574] Trial 34 finished with value: 1.4969186642766989e+49 and parameters: {'n_estimators': 10, 'learning_rate': 233, 'min_weight_fraction_leaf': 0.0358367526593788, 'max_depth': 8, 'min_impurity_decrease': 516, 'min_samples_leaf': 3}. Best is trial 34 with value: 1.4969186642766989e+49.\n","[I 2024-02-12 23:38:08,365] Trial 35 finished with value: 5.938915698622523e+73 and parameters: {'n_estimators': 17, 'learning_rate': 131, 'min_weight_fraction_leaf': 0.0248197370070336, 'max_depth': 8, 'min_impurity_decrease': 541, 'min_samples_leaf': 3}. Best is trial 34 with value: 1.4969186642766989e+49.\n","[I 2024-02-12 23:38:26,729] Trial 36 finished with value: 9.318292458803765e+292 and parameters: {'n_estimators': 70, 'learning_rate': 121, 'min_weight_fraction_leaf': 0.030816434413318716, 'max_depth': 13, 'min_impurity_decrease': 619, 'min_samples_leaf': 2}. Best is trial 34 with value: 1.4969186642766989e+49.\n","[I 2024-02-12 23:38:32,733] Trial 37 finished with value: 9.175994817857972e+160 and parameters: {'n_estimators': 34, 'learning_rate': 220, 'min_weight_fraction_leaf': 0.0989926194902703, 'max_depth': 9, 'min_impurity_decrease': 674, 'min_samples_leaf': 3}. Best is trial 34 with value: 1.4969186642766989e+49.\n","[I 2024-02-12 23:38:34,675] Trial 38 finished with value: 1.3882818777581342e+143 and parameters: {'n_estimators': 28, 'learning_rate': 338, 'min_weight_fraction_leaf': 0.39921182721975723, 'max_depth': 7, 'min_impurity_decrease': 373, 'min_samples_leaf': 1}. Best is trial 34 with value: 1.4969186642766989e+49.\n","[I 2024-02-12 23:38:45,307] Trial 39 finished with value: 1.1050464000575039e+248 and parameters: {'n_estimators': 48, 'learning_rate': 368, 'min_weight_fraction_leaf': 0.06301895083437838, 'max_depth': 21, 'min_impurity_decrease': 486, 'min_samples_leaf': 6}. Best is trial 34 with value: 1.4969186642766989e+49.\n","[I 2024-02-12 23:38:47,235] Trial 40 finished with value: 5.1217758968775245e+119 and parameters: {'n_estimators': 22, 'learning_rate': 484, 'min_weight_fraction_leaf': 0.21953566404106306, 'max_depth': 6, 'min_impurity_decrease': 700, 'min_samples_leaf': 2}. Best is trial 34 with value: 1.4969186642766989e+49.\n","[I 2024-02-12 23:38:48,906] Trial 41 finished with value: 2.6343800202881447e+48 and parameters: {'n_estimators': 10, 'learning_rate': 216, 'min_weight_fraction_leaf': 0.10859358454459445, 'max_depth': 8, 'min_impurity_decrease': 438, 'min_samples_leaf': 4}. Best is trial 41 with value: 2.6343800202881447e+48.\n","[I 2024-02-12 23:38:51,610] Trial 42 finished with value: 7.803066812099725e+73 and parameters: {'n_estimators': 16, 'learning_rate': 180, 'min_weight_fraction_leaf': 0.10617829824421528, 'max_depth': 9, 'min_impurity_decrease': 428, 'min_samples_leaf': 4}. Best is trial 41 with value: 2.6343800202881447e+48.\n","[I 2024-02-12 23:38:55,625] Trial 43 finished with value: 4.6280493332023166e+73 and parameters: {'n_estimators': 15, 'learning_rate': 248, 'min_weight_fraction_leaf': 0.031152129385487858, 'max_depth': 11, 'min_impurity_decrease': 547, 'min_samples_leaf': 5}. Best is trial 41 with value: 2.6343800202881447e+48.\n","[I 2024-02-12 23:38:57,850] Trial 44 finished with value: 4.8889081554214523e+54 and parameters: {'n_estimators': 10, 'learning_rate': 441, 'min_weight_fraction_leaf': 0.07434942691672458, 'max_depth': 16, 'min_impurity_decrease': 770, 'min_samples_leaf': 4}. Best is trial 41 with value: 2.6343800202881447e+48.\n","[I 2024-02-12 23:38:59,975] Trial 45 finished with value: 2.6510858453333463e+54 and parameters: {'n_estimators': 10, 'learning_rate': 427, 'min_weight_fraction_leaf': 0.0669771737718741, 'max_depth': 32, 'min_impurity_decrease': 780, 'min_samples_leaf': 4}. Best is trial 41 with value: 2.6343800202881447e+48.\n","[I 2024-02-12 23:39:04,336] Trial 46 finished with value: 2.1573210553911427e+74 and parameters: {'n_estimators': 14, 'learning_rate': 387, 'min_weight_fraction_leaf': 0.01581006598265848, 'max_depth': 22, 'min_impurity_decrease': 829, 'min_samples_leaf': 3}. Best is trial 41 with value: 2.6343800202881447e+48.\n","[I 2024-02-12 23:39:11,225] Trial 47 finished with value: 4.2555526039889905e+122 and parameters: {'n_estimators': 30, 'learning_rate': 104, 'min_weight_fraction_leaf': 0.04162674792516299, 'max_depth': 32, 'min_impurity_decrease': 983, 'min_samples_leaf': 5}. Best is trial 41 with value: 2.6343800202881447e+48.\n","[I 2024-02-12 23:39:14,417] Trial 48 finished with value: 3.664208956759086e+114 and parameters: {'n_estimators': 25, 'learning_rate': 182, 'min_weight_fraction_leaf': 0.16909382975435416, 'max_depth': 14, 'min_impurity_decrease': 325, 'min_samples_leaf': 5}. Best is trial 41 with value: 2.6343800202881447e+48.\n","[I 2024-02-12 23:39:17,826] Trial 49 finished with value: 8.796001729685882e+19 and parameters: {'n_estimators': 19, 'learning_rate': 4, 'min_weight_fraction_leaf': 0.09366801634802481, 'max_depth': 18, 'min_impurity_decrease': 880, 'min_samples_leaf': 3}. Best is trial 49 with value: 8.796001729685882e+19.\n","[I 2024-02-12 23:39:20,221] Trial 50 finished with value: 5.157950099981395e+72 and parameters: {'n_estimators': 19, 'learning_rate': 75, 'min_weight_fraction_leaf': 0.14582520862268683, 'max_depth': 17, 'min_impurity_decrease': 253, 'min_samples_leaf': 2}. Best is trial 49 with value: 8.796001729685882e+19.\n","[I 2024-02-12 23:39:23,133] Trial 51 finished with value: 6.878286514808079e+65 and parameters: {'n_estimators': 13, 'learning_rate': 291, 'min_weight_fraction_leaf': 0.07875467943164247, 'max_depth': 18, 'min_impurity_decrease': 818, 'min_samples_leaf': 3}. Best is trial 49 with value: 8.796001729685882e+19.\n","[I 2024-02-12 23:39:25,542] Trial 52 finished with value: 1.1684495997027919e+55 and parameters: {'n_estimators': 10, 'learning_rate': 461, 'min_weight_fraction_leaf': 0.0985350629385215, 'max_depth': 32, 'min_impurity_decrease': 915, 'min_samples_leaf': 4}. Best is trial 49 with value: 8.796001729685882e+19.\n","[I 2024-02-12 23:39:30,245] Trial 53 finished with value: 1.543775150794539e+78 and parameters: {'n_estimators': 14, 'learning_rate': 531, 'min_weight_fraction_leaf': 0.01691169390281362, 'max_depth': 26, 'min_impurity_decrease': 745, 'min_samples_leaf': 6}. Best is trial 49 with value: 8.796001729685882e+19.\n","[I 2024-02-12 23:39:34,232] Trial 54 finished with value: 2.5449849333831276e+31 and parameters: {'n_estimators': 19, 'learning_rate': 7, 'min_weight_fraction_leaf': 0.055766620887255675, 'max_depth': 24, 'min_impurity_decrease': 872, 'min_samples_leaf': 4}. Best is trial 49 with value: 8.796001729685882e+19.\n","[I 2024-02-12 23:39:37,434] Trial 55 finished with value: 4.612870353427897e+45 and parameters: {'n_estimators': 23, 'learning_rate': 10, 'min_weight_fraction_leaf': 0.136412183266309, 'max_depth': 24, 'min_impurity_decrease': 932, 'min_samples_leaf': 3}. Best is trial 49 with value: 8.796001729685882e+19.\n","[I 2024-02-12 23:39:40,737] Trial 56 finished with value: 1.6594008198509504e+16 and parameters: {'n_estimators': 24, 'learning_rate': 3, 'min_weight_fraction_leaf': 0.1300563382794123, 'max_depth': 19, 'min_impurity_decrease': 930, 'min_samples_leaf': 3}. Best is trial 56 with value: 1.6594008198509504e+16.\n","[I 2024-02-12 23:39:45,772] Trial 57 finished with value: 1.3112253763284108e+45 and parameters: {'n_estimators': 36, 'learning_rate': 5, 'min_weight_fraction_leaf': 0.1417079306193866, 'max_depth': 22, 'min_impurity_decrease': 922, 'min_samples_leaf': 2}. Best is trial 56 with value: 1.6594008198509504e+16.\n","[I 2024-02-12 23:39:51,107] Trial 58 finished with value: 12.585749513002074 and parameters: {'n_estimators': 38, 'learning_rate': 1, 'min_weight_fraction_leaf': 0.13820466414904697, 'max_depth': 23, 'min_impurity_decrease': 893, 'min_samples_leaf': 1}. Best is trial 58 with value: 12.585749513002074.\n","[I 2024-02-12 23:39:54,397] Trial 59 finished with value: 100.80179739707374 and parameters: {'n_estimators': 37, 'learning_rate': 0, 'min_weight_fraction_leaf': 0.22628442994012116, 'max_depth': 24, 'min_impurity_decrease': 932, 'min_samples_leaf': 1}. Best is trial 58 with value: 12.585749513002074.\n","[I 2024-02-12 23:39:58,566] Trial 60 finished with value: 2.4134954704332357e+93 and parameters: {'n_estimators': 39, 'learning_rate': 16, 'min_weight_fraction_leaf': 0.18211292409108185, 'max_depth': 27, 'min_impurity_decrease': 877, 'min_samples_leaf': 1}. Best is trial 58 with value: 12.585749513002074.\n","[I 2024-02-12 23:40:01,713] Trial 61 finished with value: 4.7322027944772105e+93 and parameters: {'n_estimators': 36, 'learning_rate': 20, 'min_weight_fraction_leaf': 0.22689408614561088, 'max_depth': 23, 'min_impurity_decrease': 925, 'min_samples_leaf': 1}. Best is trial 58 with value: 12.585749513002074.\n","[I 2024-02-12 23:40:07,554] Trial 62 finished with value: 100.80179739707374 and parameters: {'n_estimators': 43, 'learning_rate': 0, 'min_weight_fraction_leaf': 0.13476658807009217, 'max_depth': 19, 'min_impurity_decrease': 947, 'min_samples_leaf': 2}. Best is trial 58 with value: 12.585749513002074.\n","[I 2024-02-12 23:40:10,595] Trial 63 finished with value: 4.2887738547443533e+172 and parameters: {'n_estimators': 47, 'learning_rate': 67, 'min_weight_fraction_leaf': 0.28968687270700183, 'max_depth': 19, 'min_impurity_decrease': 950, 'min_samples_leaf': 2}. Best is trial 58 with value: 12.585749513002074.\n","[I 2024-02-12 23:40:15,379] Trial 64 finished with value: 1.1045129342975968e+174 and parameters: {'n_estimators': 44, 'learning_rate': 92, 'min_weight_fraction_leaf': 0.2105084461894854, 'max_depth': 20, 'min_impurity_decrease': 859, 'min_samples_leaf': 2}. Best is trial 58 with value: 12.585749513002074.\n","[I 2024-02-12 23:40:20,482] Trial 65 finished with value: 1.8956159263794952e+140 and parameters: {'n_estimators': 41, 'learning_rate': 50, 'min_weight_fraction_leaf': 0.1462950651290252, 'max_depth': 28, 'min_impurity_decrease': 877, 'min_samples_leaf': 1}. Best is trial 58 with value: 12.585749513002074.\n","[I 2024-02-12 23:40:25,266] Trial 66 finished with value: 4.7323431866312074e+235 and parameters: {'n_estimators': 54, 'learning_rate': 148, 'min_weight_fraction_leaf': 0.2456854842882145, 'max_depth': 29, 'min_impurity_decrease': 998, 'min_samples_leaf': 2}. Best is trial 58 with value: 12.585749513002074.\n","[I 2024-02-12 23:40:29,125] Trial 67 finished with value: 1.0379866247715096e+45 and parameters: {'n_estimators': 31, 'learning_rate': 6, 'min_weight_fraction_leaf': 0.17416514739890762, 'max_depth': 24, 'min_impurity_decrease': 959, 'min_samples_leaf': 1}. Best is trial 58 with value: 12.585749513002074.\n","[I 2024-02-12 23:40:32,797] Trial 68 finished with value: 3.598703191598415e+102 and parameters: {'n_estimators': 30, 'learning_rate': 49, 'min_weight_fraction_leaf': 0.15949279887076595, 'max_depth': 25, 'min_impurity_decrease': 961, 'min_samples_leaf': 1}. Best is trial 58 with value: 12.585749513002074.\n","[I 2024-02-12 23:40:37,139] Trial 69 finished with value: 8.41103167704549e+88 and parameters: {'n_estimators': 28, 'learning_rate': 37, 'min_weight_fraction_leaf': 0.12321009881894626, 'max_depth': 19, 'min_impurity_decrease': 842, 'min_samples_leaf': 1}. Best is trial 58 with value: 12.585749513002074.\n","[I 2024-02-12 23:40:40,598] Trial 70 finished with value: 2.941211083098383e+124 and parameters: {'n_estimators': 32, 'learning_rate': 84, 'min_weight_fraction_leaf': 0.19874709107596591, 'max_depth': 14, 'min_impurity_decrease': 898, 'min_samples_leaf': 1}. Best is trial 58 with value: 12.585749513002074.\n","[I 2024-02-12 23:40:45,099] Trial 71 finished with value: 5.651336728131669e+22 and parameters: {'n_estimators': 35, 'learning_rate': 3, 'min_weight_fraction_leaf': 0.1740154710243841, 'max_depth': 22, 'min_impurity_decrease': 950, 'min_samples_leaf': 2}. Best is trial 58 with value: 12.585749513002074.\n","[I 2024-02-12 23:40:50,328] Trial 72 finished with value: 2.474752962041761e+60 and parameters: {'n_estimators': 42, 'learning_rate': 6, 'min_weight_fraction_leaf': 0.17527611420958225, 'max_depth': 20, 'min_impurity_decrease': 966, 'min_samples_leaf': 2}. Best is trial 58 with value: 12.585749513002074.\n","[I 2024-02-12 23:40:53,623] Trial 73 finished with value: 9.898449755676161e+163 and parameters: {'n_estimators': 38, 'learning_rate': 138, 'min_weight_fraction_leaf': 0.24465871518798724, 'max_depth': 24, 'min_impurity_decrease': 887, 'min_samples_leaf': 1}. Best is trial 58 with value: 12.585749513002074.\n","[I 2024-02-12 23:40:57,835] Trial 74 finished with value: 3.9441807813779924e+122 and parameters: {'n_estimators': 34, 'learning_rate': 61, 'min_weight_fraction_leaf': 0.15612329381838905, 'max_depth': 29, 'min_impurity_decrease': 949, 'min_samples_leaf': 2}. Best is trial 58 with value: 12.585749513002074.\n","[I 2024-02-12 23:41:01,399] Trial 75 finished with value: 100.80179739707374 and parameters: {'n_estimators': 26, 'learning_rate': 0, 'min_weight_fraction_leaf': 0.12950754408921222, 'max_depth': 16, 'min_impurity_decrease': 854, 'min_samples_leaf': 3}. Best is trial 58 with value: 12.585749513002074.\n","[I 2024-02-12 23:41:04,989] Trial 76 finished with value: 1.644248687017692e+106 and parameters: {'n_estimators': 26, 'learning_rate': 103, 'min_weight_fraction_leaf': 0.12678468034896587, 'max_depth': 17, 'min_impurity_decrease': 849, 'min_samples_leaf': 3}. Best is trial 58 with value: 12.585749513002074.\n","[I 2024-02-12 23:41:13,020] Trial 77 finished with value: 3.019480406154175e+138 and parameters: {'n_estimators': 45, 'learning_rate': 34, 'min_weight_fraction_leaf': 0.08973377469550402, 'max_depth': 15, 'min_impurity_decrease': 807, 'min_samples_leaf': 3}. Best is trial 58 with value: 12.585749513002074.\n","[I 2024-02-12 23:41:16,167] Trial 78 finished with value: 1.2083420067079233e+133 and parameters: {'n_estimators': 36, 'learning_rate': 68, 'min_weight_fraction_leaf': 0.2707001540579593, 'max_depth': 16, 'min_impurity_decrease': 992, 'min_samples_leaf': 3}. Best is trial 58 with value: 12.585749513002074.\n","[I 2024-02-12 23:41:20,078] Trial 79 finished with value: 8.580572456229466e+283 and parameters: {'n_estimators': 64, 'learning_rate': 166, 'min_weight_fraction_leaf': 0.49304218404127254, 'max_depth': 21, 'min_impurity_decrease': 788, 'min_samples_leaf': 2}. Best is trial 58 with value: 12.585749513002074.\n","[I 2024-02-12 23:41:23,033] Trial 80 finished with value: 5.252792604270499e+112 and parameters: {'n_estimators': 27, 'learning_rate': 115, 'min_weight_fraction_leaf': 0.18662867222805335, 'max_depth': 18, 'min_impurity_decrease': 900, 'min_samples_leaf': 3}. Best is trial 58 with value: 12.585749513002074.\n","[I 2024-02-12 23:41:26,871] Trial 81 finished with value: 1.0022370244897398e+95 and parameters: {'n_estimators': 31, 'learning_rate': 33, 'min_weight_fraction_leaf': 0.16578580956360525, 'max_depth': 22, 'min_impurity_decrease': 943, 'min_samples_leaf': 1}. Best is trial 58 with value: 12.585749513002074.\n","[I 2024-02-12 23:41:30,217] Trial 82 finished with value: 12.512202250709457 and parameters: {'n_estimators': 20, 'learning_rate': 1, 'min_weight_fraction_leaf': 0.11423580927305693, 'max_depth': 26, 'min_impurity_decrease': 862, 'min_samples_leaf': 2}. Best is trial 82 with value: 12.512202250709457.\n","[I 2024-02-12 23:41:33,453] Trial 83 finished with value: 6.2650081174928e+70 and parameters: {'n_estimators': 21, 'learning_rate': 45, 'min_weight_fraction_leaf': 0.11649701924951783, 'max_depth': 27, 'min_impurity_decrease': 860, 'min_samples_leaf': 2}. Best is trial 82 with value: 12.512202250709457.\n","[I 2024-02-12 23:41:36,668] Trial 84 finished with value: 100.14338957178055 and parameters: {'n_estimators': 23, 'learning_rate': 2, 'min_weight_fraction_leaf': 0.13100179883918747, 'max_depth': 20, 'min_impurity_decrease': 752, 'min_samples_leaf': 2}. Best is trial 82 with value: 12.512202250709457.\n","[I 2024-02-12 23:41:39,988] Trial 85 finished with value: 2.3770563562693086e+93 and parameters: {'n_estimators': 24, 'learning_rate': 82, 'min_weight_fraction_leaf': 0.12993962635188303, 'max_depth': 19, 'min_impurity_decrease': 904, 'min_samples_leaf': 2}. Best is trial 82 with value: 12.512202250709457.\n","[I 2024-02-12 23:41:42,788] Trial 86 finished with value: 4.731588789749487e+66 and parameters: {'n_estimators': 22, 'learning_rate': 31, 'min_weight_fraction_leaf': 0.1541457418609069, 'max_depth': 21, 'min_impurity_decrease': 801, 'min_samples_leaf': 2}. Best is trial 82 with value: 12.512202250709457.\n","[I 2024-02-12 23:41:48,164] Trial 87 finished with value: 3.815779119065405e+181 and parameters: {'n_estimators': 50, 'learning_rate': 64, 'min_weight_fraction_leaf': 0.19724385087805293, 'max_depth': 12, 'min_impurity_decrease': 751, 'min_samples_leaf': 2}. Best is trial 82 with value: 12.512202250709457.\n","[I 2024-02-12 23:41:52,030] Trial 88 finished with value: 5.279898488408354e+114 and parameters: {'n_estimators': 28, 'learning_rate': 105, 'min_weight_fraction_leaf': 0.1353873813483195, 'max_depth': 16, 'min_impurity_decrease': 728, 'min_samples_leaf': 1}. Best is trial 82 with value: 12.512202250709457.\n","C:\\Users\\valentin\\AppData\\Roaming\\Python\\Python311\\site-packages\\numpy\\core\\_methods.py:49: RuntimeWarning: overflow encountered in reduce\n","  return umr_sum(a, axis, dtype, out, keepdims, initial, where)\n","C:\\Users\\valentin\\AppData\\Roaming\\Python\\Python311\\site-packages\\numpy\\core\\_methods.py:49: RuntimeWarning: overflow encountered in reduce\n","  return umr_sum(a, axis, dtype, out, keepdims, initial, where)\n","C:\\Users\\valentin\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\metrics\\_regression.py:501: RuntimeWarning: overflow encountered in square\n","  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)\n","[I 2024-02-12 23:42:09,426] Trial 89 finished with value: inf and parameters: {'n_estimators': 88, 'learning_rate': 155, 'min_weight_fraction_leaf': 0.09653258242462481, 'max_depth': 14, 'min_impurity_decrease': 843, 'min_samples_leaf': 3}. Best is trial 82 with value: 12.512202250709457.\n","[I 2024-02-12 23:42:11,473] Trial 90 finished with value: 1.9672348349921428e+54 and parameters: {'n_estimators': 18, 'learning_rate': 30, 'min_weight_fraction_leaf': 0.20947650312383795, 'max_depth': 18, 'min_impurity_decrease': 978, 'min_samples_leaf': 2}. Best is trial 82 with value: 12.512202250709457.\n","[I 2024-02-12 23:42:14,651] Trial 91 finished with value: 7.207325539434706e+20 and parameters: {'n_estimators': 20, 'learning_rate': 4, 'min_weight_fraction_leaf': 0.11036047635547327, 'max_depth': 30, 'min_impurity_decrease': 866, 'min_samples_leaf': 3}. Best is trial 82 with value: 12.512202250709457.\n","[I 2024-02-12 23:42:19,938] Trial 92 finished with value: 1.483882152491657e+121 and parameters: {'n_estimators': 34, 'learning_rate': 58, 'min_weight_fraction_leaf': 0.11141961818448728, 'max_depth': 30, 'min_impurity_decrease': 913, 'min_samples_leaf': 3}. Best is trial 82 with value: 12.512202250709457.\n","[I 2024-02-12 23:42:21,213] Trial 93 finished with value: 100.80179739707374 and parameters: {'n_estimators': 20, 'learning_rate': 0, 'min_weight_fraction_leaf': 0.34543649525052467, 'max_depth': 25, 'min_impurity_decrease': 835, 'min_samples_leaf': 3}. Best is trial 82 with value: 12.512202250709457.\n","[I 2024-02-12 23:42:22,732] Trial 94 finished with value: 1.8498798056481152e+91 and parameters: {'n_estimators': 23, 'learning_rate': 90, 'min_weight_fraction_leaf': 0.3513515120223564, 'max_depth': 27, 'min_impurity_decrease': 836, 'min_samples_leaf': 3}. Best is trial 82 with value: 12.512202250709457.\n","[I 2024-02-12 23:42:23,823] Trial 95 finished with value: 5.892539040912965e+52 and parameters: {'n_estimators': 17, 'learning_rate': 33, 'min_weight_fraction_leaf': 0.31884638850986385, 'max_depth': 26, 'min_impurity_decrease': 813, 'min_samples_leaf': 4}. Best is trial 82 with value: 12.512202250709457.\n","[I 2024-02-12 23:42:25,135] Trial 96 finished with value: 100.80179739707606 and parameters: {'n_estimators': 20, 'learning_rate': 2, 'min_weight_fraction_leaf': 0.44610971204708744, 'max_depth': 30, 'min_impurity_decrease': 885, 'min_samples_leaf': 3}. Best is trial 82 with value: 12.512202250709457.\n","[I 2024-02-12 23:42:26,727] Trial 97 finished with value: 1.0104068761010024e+105 and parameters: {'n_estimators': 25, 'learning_rate': 118, 'min_weight_fraction_leaf': 0.3914775490405352, 'max_depth': 25, 'min_impurity_decrease': 890, 'min_samples_leaf': 3}. Best is trial 82 with value: 12.512202250709457.\n","[I 2024-02-12 23:42:27,777] Trial 98 finished with value: 1.04320763952209e+57 and parameters: {'n_estimators': 16, 'learning_rate': 55, 'min_weight_fraction_leaf': 0.4316770565362984, 'max_depth': 23, 'min_impurity_decrease': 925, 'min_samples_leaf': 4}. Best is trial 82 with value: 12.512202250709457.\n","[I 2024-02-12 23:42:29,066] Trial 99 finished with value: 3.924933615548256e+52 and parameters: {'n_estimators': 19, 'learning_rate': 23, 'min_weight_fraction_leaf': 0.4530343462989667, 'max_depth': 20, 'min_impurity_decrease': 765, 'min_samples_leaf': 2}. Best is trial 82 with value: 12.512202250709457.\n"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Meilleurs Paramètres: {'n_estimators': 20, 'learning_rate': 1, 'min_weight_fraction_leaf': 0.11423580927305693, 'max_depth': 26, 'min_impurity_decrease': 862, 'min_samples_leaf': 2}\n","Mean Squared Error: 12.427460764326584\n","R-squared: 0.8763389598541953\n","\n"," Importances des variables :\n","                 1         4          2         0         3\n","Variable  Latitude     Month  Longitude   Country      Year\n","Poids     0.585932  0.325782   0.046303  0.040769  0.001214\n"]}],"source":["# Définition de la fonction objectif pour Optuna\n","def objectif(trial):\n","    # Définition de l'espace de recherche des hyperparamètres\n","    params = {\n","        'n_estimators': trial.suggest_int('n_estimators', 10, 100),\n","        'learning_rate': trial.suggest_int('learning_rate', 0, 1000),\n","        'min_weight_fraction_leaf': trial.suggest_float('min_weight_fraction_leaf', 0.0, 0.5),\n","        'max_depth': trial.suggest_int('max_depth', 2, 32, log=True),\n","        'min_impurity_decrease': trial.suggest_int('min_impurity_decrease', 0, 1000),\n","        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n","    }\n","\n","    # Création du modèle avec les hyperparamètres choisis\n","    model = GradientBoostingRegressor(**params)\n","\n","    # Entraînement du modèle\n","    model.fit(X_train, y_train)\n","\n","    # Prédictions sur l'ensemble de validation\n","    y_pred = model.predict(X_test)\n","\n","    # Retourne la MSE négative car Optuna cherche à minimiser l'objectif\n","    return mean_squared_error(y_test, y_pred)\n","\n","# Création d'une étude Optuna pour minimiser la MSE\n","studyGBR = optuna.create_study(direction='minimize')\n","studyGBR.optimize(objectif, n_trials=100)\n","\n","# Récupération des meilleurs paramètres de l'étude\n","meilleurs_params = studyGBR.best_params\n","meilleur_trial = studyGBR.best_trial\n","\n","print('\\n')\n","print(\"Meilleurs Paramètres:\", meilleurs_params)\n","\n","# Création d'un modèle avec les hyperparamètres optimaux\n","meilleur_model = GradientBoostingRegressor(**meilleurs_params)\n","meilleur_model.fit(X_train, y_train)\n","y_pred = meilleur_model.predict(X)\n","\n","# Calcul du MSE et du R2 avec les hyperparamètres optimaux\n","mseGBR = mean_squared_error(y, y_pred)\n","r2GBR = r2_score(y, y_pred)\n","\n","print(\"Mean Squared Error:\", mseGBR)\n","print(\"R-squared:\", r2GBR)\n","\n","# Accéder aux poids des différentes variables\n","importances = meilleur_model.feature_importances_\n","\n","# Créer une liste des noms de variables\n","noms_variables = list(X_train.columns) \n","\n","# Associer les poids aux noms de variables\n","importance_df = pd.DataFrame({'Variable': X_train.columns, 'Poids': importances})\n","importance_df = importance_df.sort_values('Poids', ascending=False)\n","\n","# Afficher les poids des différentes variables\n","print(\"\\n Importances des variables :\")\n","importance=importance_df.T\n","print(importance.to_string())\n","\n","\n","with open('studyGBR.pkl', 'wb') as f:\n","    pickle.dump(studyGBR, f)\n","    \n","with open('studyGBR.pkl', 'rb') as f:\n","    studyGBR = pickle.load(f)\n"]},{"cell_type":"markdown","metadata":{},"source":["# Tests -> Time Series"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from statsmodels.tsa.seasonal import seasonal_decompose\n","\n","# Créer une série temporelle fictive\n","np.random.seed(0)\n","dates = ByMajorCity_true['dt']\n","values = ByMajorCity_true['AverageTemperature']\n","df = pd.DataFrame({'date': dates, 'votre_variable': values})\n","\n","# Décomposer la série temporelle\n","result = seasonal_decompose(df['votre_variable'], model='additive', period=365)\n","\n","# Afficher les composantes décomposées\n","# Élargir la figure\n","fig, axes = plt.subplots(ncols=1, nrows=4, sharex=True, figsize=(20, 8))  # Ajustez la taille selon vos besoins\n","\n","# Afficher les composantes décomposées\n","result.observed.plot(ax=axes[0], legend=False)\n","axes[0].set_ylabel('Observed')\n","\n","result.trend.plot(ax=axes[1], legend=False)\n","axes[1].set_ylabel('Trend')\n","\n","result.seasonal.plot(ax=axes[2], legend=False)\n","axes[2].set_ylabel('Seasonal')\n","\n","result.resid.plot(ax=axes[3], legend=False)\n","axes[3].set_ylabel('Residual')\n","\n","plt.tight_layout()  \n","plt.show()"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.7"}},"nbformat":4,"nbformat_minor":2}
